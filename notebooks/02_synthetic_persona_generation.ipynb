{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-intro",
   "metadata": {},
   "source": [
    "# 02 - Synthetic Persona Generation\n",
    "\n",
    "This notebook develops the workflow for creating synthetic personas:\n",
    "1. Extract context from personal data files using LLM analysis\n",
    "2. Generate synthetic personas with realistic data files\n",
    "3. Document LLM biases and workarounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from centuria.models import Persona, Question, Survey\n",
    "from centuria.persona import create_persona, SyntheticIdentity, generate_synthetic_files, list_available_file_types\n",
    "from centuria.survey import ask_question, run_survey\n",
    "from centuria.data import load_files, process_personal_folder\n",
    "from centuria.llm import complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Context Extraction from Real Data\n",
    "\n",
    "Extract a structured profile from personal data files, then synthesize a context statement.\n",
    "\n",
    "The prompts include anti-sycophancy rules to avoid flattering language (\"accomplished\", \"key player\", etc). See `centuria/data/extractors.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-extract-profile",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "litellm.RateLimitError: RateLimitError: OpenAIException - You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/artificial-centuria/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:840\u001b[39m, in \u001b[36mOpenAIChatCompletion.acompletion\u001b[39m\u001b[34m(self, messages, optional_params, litellm_params, provider_config, model, model_response, logging_obj, timeout, api_key, api_base, api_version, organization, client, max_retries, headers, drop_params, stream_options, fake_stream, shared_session)\u001b[39m\n\u001b[32m    827\u001b[39m logging_obj.pre_call(\n\u001b[32m    828\u001b[39m     \u001b[38;5;28minput\u001b[39m=data[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    829\u001b[39m     api_key=openai_aclient.api_key,\n\u001b[32m   (...)\u001b[39m\u001b[32m    837\u001b[39m     },\n\u001b[32m    838\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m840\u001b[39m headers, response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.make_openai_chat_completion_request(\n\u001b[32m    841\u001b[39m     openai_aclient=openai_aclient,\n\u001b[32m    842\u001b[39m     data=data,\n\u001b[32m    843\u001b[39m     timeout=timeout,\n\u001b[32m    844\u001b[39m     logging_obj=logging_obj,\n\u001b[32m    845\u001b[39m )\n\u001b[32m    846\u001b[39m stringified_response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/artificial-centuria/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_utils.py:190\u001b[39m, in \u001b[36mtrack_llm_api_timing.<locals>.decorator.<locals>.async_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/artificial-centuria/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:460\u001b[39m, in \u001b[36mOpenAIChatCompletion.make_openai_chat_completion_request\u001b[39m\u001b[34m(self, openai_aclient, data, timeout, logging_obj)\u001b[39m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/artificial-centuria/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:437\u001b[39m, in \u001b[36mOpenAIChatCompletion.make_openai_chat_completion_request\u001b[39m\u001b[34m(self, openai_aclient, data, timeout, logging_obj)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    436\u001b[39m     raw_response = (\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m openai_aclient.chat.completions.with_raw_response.create(\n\u001b[32m    438\u001b[39m             **data, timeout=timeout\n\u001b[32m    439\u001b[39m         )\n\u001b[32m    440\u001b[39m     )\n\u001b[32m    441\u001b[39m     end_time = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/artificial-centuria/.venv/lib/python3.11/site-packages/openai/_legacy_response.py:381\u001b[39m, in \u001b[36masync_to_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    379\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/artificial-centuria/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:2678\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2677\u001b[39m validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2678\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2679\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2680\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2681\u001b[39m         {\n\u001b[32m   2682\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2683\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2684\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2685\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2686\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2687\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2688\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2689\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2690\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2691\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2692\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2693\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2694\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2695\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2696\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2697\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2698\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2699\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_retention\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_retention,\n\u001b[32m   2700\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2701\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2702\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2703\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2704\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2705\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2706\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2707\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2708\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2709\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2710\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2711\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2712\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2713\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2714\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2715\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   2716\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2717\u001b[39m         },\n\u001b[32m   2718\u001b[39m         completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2719\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2720\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2721\u001b[39m     ),\n\u001b[32m   2722\u001b[39m     options=make_request_options(\n\u001b[32m   2723\u001b[39m         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2724\u001b[39m     ),\n\u001b[32m   2725\u001b[39m     cast_to=ChatCompletion,\n\u001b[32m   2726\u001b[39m     stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2727\u001b[39m     stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2728\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/artificial-centuria/.venv/lib/python3.11/site-packages/openai/_base_client.py:1797\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1794\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1795\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1796\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1797\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/artificial-centuria/.venv/lib/python3.11/site-packages/openai/_base_client.py:1597\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1596\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1597\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1599\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/artificial-centuria/.venv/lib/python3.11/site-packages/litellm/main.py:609\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, verbosity, safety_identifier, service_tier, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, shared_session, **kwargs)\u001b[39m\n\u001b[32m    608\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m asyncio.iscoroutine(init_response):\n\u001b[32m--> \u001b[39m\u001b[32m609\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m init_response\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/artificial-centuria/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py:887\u001b[39m, in \u001b[36mOpenAIChatCompletion.acompletion\u001b[39m\u001b[34m(self, messages, optional_params, litellm_params, provider_config, model, model_response, logging_obj, timeout, api_key, api_base, api_version, organization, client, max_retries, headers, drop_params, stream_options, fake_stream, shared_session)\u001b[39m\n\u001b[32m    885\u001b[39m message = \u001b[38;5;28mgetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    888\u001b[39m     status_code=status_code,\n\u001b[32m    889\u001b[39m     message=message,\n\u001b[32m    890\u001b[39m     headers=error_headers,\n\u001b[32m    891\u001b[39m     body=exception_body,\n\u001b[32m    892\u001b[39m )\n",
      "\u001b[31mOpenAIError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m profile, context_statement = \u001b[38;5;28;01mawait\u001b[39;00m process_personal_folder(\u001b[33m'\u001b[39m\u001b[33m../data/personal\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mName: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprofile.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRole: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprofile.current_role\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprofile.industry\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/artificial-centuria/notebooks/../src/centuria/data/extractors.py:202\u001b[39m, in \u001b[36mprocess_personal_folder\u001b[39m\u001b[34m(folder_path)\u001b[39m\n\u001b[32m    199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo valid files found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# Extract profile and build context\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m profile = \u001b[38;5;28;01mawait\u001b[39;00m extract_profile_from_files(files)\n\u001b[32m    203\u001b[39m context_statement = \u001b[38;5;28;01mawait\u001b[39;00m build_context_statement(profile)\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m profile, context_statement\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/artificial-centuria/notebooks/../src/centuria/data/extractors.py:129\u001b[39m, in \u001b[36mextract_profile_from_files\u001b[39m\u001b[34m(paths)\u001b[39m\n\u001b[32m    126\u001b[39m     sections.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m=== \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcontent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    128\u001b[39m combined = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(sections)\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m extract_profile_from_text(combined)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/artificial-centuria/notebooks/../src/centuria/data/extractors.py:99\u001b[39m, in \u001b[36mextract_profile_from_text\u001b[39m\u001b[34m(content)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m     98\u001b[39m prompt = EXTRACTION_PROMPT.format(content=content)\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m complete(prompt)\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Parse the JSON response\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    103\u001b[39m     \u001b[38;5;66;03m# Handle potential markdown code blocks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/artificial-centuria/notebooks/../src/centuria/llm/client.py:108\u001b[39m, in \u001b[36mcomplete\u001b[39m\u001b[34m(prompt, system, model)\u001b[39m\n\u001b[32m    105\u001b[39m     messages.append({\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: system})\n\u001b[32m    106\u001b[39m messages.append({\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt})\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m litellm.acompletion(model=model, messages=messages)\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# Calculate cost using litellm's built-in pricing\u001b[39;00m\n\u001b[32m    111\u001b[39m cost = litellm.completion_cost(completion_response=response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/artificial-centuria/.venv/lib/python3.11/site-packages/litellm/utils.py:1915\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1913\u001b[39m timeout = _get_wrapper_timeout(kwargs=kwargs, exception=e)\n\u001b[32m   1914\u001b[39m \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m, timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1915\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/artificial-centuria/.venv/lib/python3.11/site-packages/litellm/utils.py:1759\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1756\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1759\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m original_function(*args, **kwargs)\n\u001b[32m   1760\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1761\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[32m   1762\u001b[39m     kwargs=kwargs,\n\u001b[32m   1763\u001b[39m     call_type=call_type,\n\u001b[32m   1764\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/artificial-centuria/.venv/lib/python3.11/site-packages/litellm/main.py:628\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, verbosity, safety_identifier, service_tier, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, shared_session, **kwargs)\u001b[39m\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    627\u001b[39m     custom_llm_provider = custom_llm_provider \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/artificial-centuria/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2346\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2344\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2346\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2347\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2348\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm.LITELLM_EXCEPTION_TYPES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/artificial-centuria/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:365\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ExceptionCheckers.is_error_str_rate_limit(error_str):\n\u001b[32m    364\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\n\u001b[32m    366\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRateLimitError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    367\u001b[39m         model=model,\n\u001b[32m    368\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m    369\u001b[39m         response=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    370\u001b[39m     )\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m ExceptionCheckers.is_error_str_context_window_exceeded(error_str):\n\u001b[32m    372\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mRateLimitError\u001b[39m: litellm.RateLimitError: RateLimitError: OpenAIException - You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors."
     ]
    }
   ],
   "source": [
    "profile, context_statement = await process_personal_folder('../data/personal')\n",
    "\n",
    "print(f\"Name: {profile.name}\")\n",
    "print(f\"Role: {profile.current_role} ({profile.industry})\")\n",
    "print(f\"Political lean: {profile.political_lean}\")\n",
    "print(f\"\\nContext statement ({len(context_statement)} chars):\")\n",
    "print(context_statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: LLM Bias in Identity Generation\n",
    "\n",
    "When asked to generate random people, LLMs default to educated, progressive, urban professionals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-naive-gen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive prompt without debiasing\n",
    "NAIVE_PROMPT = \"\"\"Generate a realistic synthetic identity. Return JSON:\n",
    "{\"name\": \"Full Name\", \"age\": \"integer\", \"gender\": \"string\", \"location\": \"City, Country\",\n",
    " \"occupation\": \"Job\", \"industry\": \"Industry\", \"education\": \"Education level\",\n",
    " \"political_lean\": \"Political orientation\", \"personality_sketch\": \"2-3 sentences\"}\n",
    "Return ONLY the JSON.\"\"\"\n",
    "\n",
    "print(\"Generating 5 identities with NAIVE prompt...\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    result = await complete(NAIVE_PROMPT)\n",
    "    text = result.content.strip()\n",
    "    if text.startswith(\"```\"): text = text.split(\"```\")[1].replace(\"json\", \"\", 1).strip()\n",
    "    p = json.loads(text)\n",
    "    print(f\"{i+1}. {p['name']}, {p['age']}, {p['gender']}\")\n",
    "    print(f\"   {p['occupation']} | {p['education']}\")\n",
    "    print(f\"   Political: {p['political_lean']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bias-results",
   "metadata": {},
   "source": [
    "**Typical results:** All female, all 35, all Master's degrees, all progressive/environmental. This is \"people who write content that trains LLMs\" - not a representative sample.\n",
    "\n",
    "### Batch Generation as Alternative\n",
    "\n",
    "Asking for N profiles at once forces the LLM to think about distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-batch-gen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 20 profiles in ONE batch...\n",
      "\n",
      " 1. Alice Smith, 24, female - waitress\n",
      "    high school diploma | liberal\n",
      " 2. John Doe, 65, male - retired teacher\n",
      "    bachelor's degree | conservative\n",
      " 3. Sarah Johnson, 36, female - nurse\n",
      "    associate degree | liberal\n",
      " 4. Tom Martinez, 51, male - truck driver\n",
      "    high school diploma | apolitical\n",
      " 5. Rachel Kim, 29, female - plumber\n",
      "    vocational training | moderate\n",
      " 6. Michael Brown, 59, male - janitor\n",
      "    some college | conservative\n",
      " 7. Emily Davis, 40, female - hairdresser\n",
      "    certification | liberal\n",
      " 8. Jacob Wilson, 31, male - bartender\n",
      "    high school diploma | apolitical\n",
      " 9. Linda Hernandez, 72, female - retired nurse\n",
      "    bachelor's degree | conservative\n",
      "10. Daniel Garcia, 44, male - electrician\n",
      "    vocational training | moderate\n",
      "11. Megan Patel, 27, female - cashier\n",
      "    high school diploma | moderate\n",
      "12. Chris Lee, 34, male - auto mechanic\n",
      "    some college | liberal\n",
      "13. Jessica White, 48, female - bus driver\n",
      "    high school diploma | apolitical\n",
      "14. Ahmed Khan, 63, male - security guard\n",
      "    some college | conservative\n",
      "15. Olivia Clark, 22, female - barista\n",
      "    high school diploma | liberal\n",
      "16. Robert Nelson, 54, male - postal worker\n",
      "    high school diploma | moderate\n",
      "17. Anna Robinson, 39, female - sales associate\n",
      "    some college | liberal\n",
      "18. Patrick Green, 50, male - construction worker\n",
      "    vocational training | conservative\n",
      "19. Emma Adams, 28, female - chef\n",
      "    associate degree | moderate\n",
      "20. Jason Rodriguez, 77, male - retired bus driver\n",
      "    high school diploma | conservative\n"
     ]
    }
   ],
   "source": [
    "BATCH_PROMPT = \"\"\"Generate 20 realistic identities representing diverse adults.\n",
    "\n",
    "Include diversity in: gender (50/50), age (18-80), education (only 35% have degrees),\n",
    "occupation (retail, trades, healthcare, transport - not just professionals),\n",
    "location (cities, suburbs, small towns), politics (conservatives, liberals, apolitical).\n",
    "\n",
    "Think of 20 random people: cashier, plumber, retired teacher, nurse, truck driver, etc.\n",
    "\n",
    "Return JSON array with: name, age, gender, location, occupation, industry, education,\n",
    "political_lean, personality_sketch. Return ONLY the JSON array.\"\"\"\n",
    "\n",
    "print(\"Generating 20 profiles in ONE batch...\\n\")\n",
    "\n",
    "result = await complete(BATCH_PROMPT)\n",
    "text = result.content.strip()\n",
    "if text.startswith(\"```\"): text = text.split(\"```\")[1].replace(\"json\", \"\", 1).strip()\n",
    "batch = json.loads(text)\n",
    "\n",
    "for i, p in enumerate(batch, 1):\n",
    "    print(f\"{i:2}. {p['name']}, {p['age']}, {p['gender']} - {p['occupation']}\")\n",
    "    print(f\"    {p['education']} | {p['political_lean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch-note",
   "metadata": {},
   "source": [
    "**Inspect manually:** Is there genuine diversity in age, education, occupation, and politics? If batch works, use it. Otherwise, statistical sampling from census data is needed.\n",
    "\n",
    "---\n",
    "## Part 3: File-First Persona Generation\n",
    "\n",
    "Recommended workflow:\n",
    "1. **Specify demographics** (from batch generation or census sampling)\n",
    "2. **Generate data files** (CV, reading list, subscriptions)\n",
    "3. **Extract persona** using the same pipeline as real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-manual-identity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available file types: ['cv', 'reading_list', 'subscriptions', 'spotify_favorites', 'twitter_bio', 'linkedin_summary', 'notes_snippet', 'bookmarks']\n",
      "\n",
      "Generating files for: Dale Hutchins, 58\n",
      "  Long-haul truck driver | High school diploma\n",
      "  Political: Conservative Republican, voted Trump, skeptical of mainstream media\n"
     ]
    }
   ],
   "source": [
    "print(\"Available file types:\", list(list_available_file_types().keys()))\n",
    "\n",
    "# Manually specify an identity - LLMs would never generate this unprompted\n",
    "identity = SyntheticIdentity(\n",
    "    name=\"Dale Hutchins\",\n",
    "    age=58,\n",
    "    gender=\"male\",\n",
    "    location=\"Tulsa, Oklahoma, USA\",\n",
    "    occupation=\"Long-haul truck driver\",\n",
    "    industry=\"Transportation\",\n",
    "    education=\"High school diploma\",\n",
    "    political_lean=\"Conservative Republican, voted Trump, skeptical of mainstream media\",\n",
    "    personality_sketch=\"Spends weeks on the road listening to talk radio. Devoted grandfather who collects model trains and grills competition brisket. Attends Baptist church when home.\"\n",
    ")\n",
    "\n",
    "output_dir = Path(f'../data/synthetic/dale_hutchins')\n",
    "print(f\"\\nGenerating files for: {identity.name}, {identity.age}\")\n",
    "print(f\"  {identity.occupation} | {identity.education}\")\n",
    "print(f\"  Political: {identity.political_lean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-generate-files",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated files:\n",
      "\n",
      "--- cv.txt ---\n",
      "```\n",
      "Dale Hutchins\n",
      "Tulsa, Oklahoma, USA\n",
      "Email: dale.hutchins58@gmail.com\n",
      "\n",
      "Professional Summary:\n",
      "Experienced long-haul truck driver with over 30 years in the transportation industry. Skilled in providing efficient and timely delivery services across the United States. Committed to safety, punctuality, and customer satisfaction while maintaining a clean driving record.\n",
      "\n",
      "Work Experience:\n",
      "\n",
      "Long-Haul Truck Driver\n",
      "ABC Logistics, Tulsa, OK\n",
      "January 2010 - Present\n",
      "- Operate a variety of commercial trucks for nationwide deliveries.\n",
      "- Maintain accurate logs and records of driving hours and vehicle inspections.\n",
      "- Ensure adherence to all Department of Transportation regulations and safety standards.\n",
      "- Consistently achieve superior customer satisfaction through timely delivery and professionalism.\n",
      "\n",
      "Truck...\n",
      "\n",
      "--- subscriptions.txt ---\n",
      "1. The Joe Rogan Experience\n",
      "2. Crime Junkie\n",
      "3. Sports Radio 1170: The Tulsa Sports Animal\n",
      "4. Country Hits Radio (e.g., SiriusXM The Highway)\n",
      "5. The BBQ Central Show (Podcast on grilling and BBQ)\n",
      "6. Netflix (for mainstream movies and TV shows)\n",
      "7. YouTube: Jason McGuinn's Model Train Channel\n",
      "8. The Daily Wire\n",
      "9. SiriusXM NASCAR Radio\n",
      "10. Comedy Central Stand-Up Audio\n",
      "\n",
      "--- recent_reads.txt ---\n",
      "1. \"The Midnight Line\" - Lee Child  \n",
      "2. \"The Old Man and the Sea\" - Ernest Hemingway  \n",
      "3. \"Spare Time: Model Railroading as a Life Hack\" - Bob Jacobson  \n",
      "4. \"Texas BBQ: A Meat-Smoking Manifesto\" - Aaron Franklin  \n",
      "5. \"Where the Crawdads Sing\" - Delia Owens  \n",
      "6. \"American Sniper: The Autobiography of the Most Lethal Sniper in U.S. Military History\" - Chris Kyle  \n",
      "7. \"Astrophysics for People in a Hurry\" - Neil deGrasse Tyson  \n",
      "8. \"The Road\" - Cormac McCarthy\n"
     ]
    }
   ],
   "source": [
    "saved_files = await generate_synthetic_files(\n",
    "    identity=identity,\n",
    "    output_dir=output_dir,\n",
    "    file_types=[\"cv\", \"subscriptions\", \"reading_list\"],\n",
    ")\n",
    "\n",
    "print(\"Generated files:\")\n",
    "for file_type, path in saved_files.items():\n",
    "    content = path.read_text()\n",
    "    print(f\"\\n--- {path.name} ---\")\n",
    "    print(content[:800] + (\"...\" if len(content) > 800 else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overfitting-note",
   "metadata": {},
   "source": [
    "**Note:** File generation prompts include anti-overfitting rules - most content should be non-political (entertainment, hobbies, practical stuff). A conservative doesn't only read conservative books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-create-persona",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created persona: Dale Hutchins\n",
      "Extracted political lean: right\n",
      "\n",
      "Context statement:\n",
      "Dale Hutchins is a long-haul truck driver based in Tulsa, Oklahoma, within the age range of 55 to 60 years. He possesses over 35 years of experience in the transportation industry, having held positions at multiple logistics companies. His career began with Tulsa Logistics from March 1988 to May 1995, followed by a role at XYZ Freight Company from June 1995 to December 2009, and he has been with ABC Logistics since January 2010. Hutchins is responsible for operating various commercial trucks for nationwide deliveries, maintaining accurate driving logs, adhering to Department of Transportation regulations, and ensuring customer satisfaction through timely and professional service.\n",
      "\n",
      "Hutchins holds a high school diploma from Union High School in Tulsa, Oklahoma, which he completed in 1983. His skill set includes a broad knowledge of DOT regulations and safety protocols, GPS navigation, route planning, and excellent communication and interpersonal abilities. He is noted for his time management and organizational skills, essential for his profession, which requires a high level of independence as well as being risk-averse.\n",
      "\n",
      "Regarding intellectual interests, Hutchins shows a diverse array of engagements. He is interested in model railroading, BBQ, and astrophysics. His media consumption includes a range of outlets from Joe Rogan's podcast to The Daily Wire, suggesting a likely right-leaning political perspective. His reading choices span multiple genres, from thrillers and classic literature to biographies and science, reflecting curiosity and a multifaceted intellectual appetite. Recent reads include titles like \"Astrophysics for People in a Hurry\" by Neil deGrasse Tyson and \"American Sniper\" by Chris Kyle, indicating an interest in scientific and military topics.\n",
      "\n",
      "Hutchins' subscriptions to various media, such as country and sports radio, podcast series focused on crime and BBQ, and services like Netflix and YouTube, highlight his wide-ranging tastes. Notably, he follows content on model railroading, which aligns with his hobby. His media preferences span mainstream and niche interests, balancing entertainment and informational content.\n",
      "\n",
      "The data suggests Hutchins places significant value on safety, punctuality, customer satisfaction, and reliability, aligning with his long-standing professional commitments. His support for transportation safety and customer service causes further underscores these values. Patterns in his career and interests indicate a dedication to delivering dependable services professionally while nurturing varied personal interests. Any views held likely align with a preference for structured, reliable environments, potentially shaped by a longstanding career focused on logistical and safety responsibilities.\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "synth_profile, synth_context = await process_personal_folder(str(output_dir))\n",
    "\n",
    "persona = create_persona(\n",
    "    name=identity.name,\n",
    "    context=synth_context,\n",
    "    persona_id=str(uuid.uuid4()),\n",
    ")\n",
    "\n",
    "print(f\"Created persona: {persona.name}\")\n",
    "print(f\"Extracted political lean: {synth_profile.political_lean}\")\n",
    "print(f\"\\nContext statement:\")\n",
    "print(synth_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-test-persona",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Dale Hutchins:\n",
      "\n",
      "Q: What's your view on the role of government in the economy?\n",
      "A: Mixed approach\n",
      "\n",
      "Q: Which party do you typically vote for?\n",
      "A: Republican\n",
      "\n",
      "Q: What motivates you most in your work?\n",
      "A: What motivates me most in my work is the satisfaction of delivering on time and knowing I’ve contributed to keeping everything running smoothly across the country. The freedom of the open road and the responsibility of ensuring cargo gets to its destination safely are rewarding. Plus, maintaining great relationships with customers and exceeding their expectations with timely deliveries really keeps me going. It's all about being reliable and efficient, just like I value in any system—be it my career, model railroading, or life in general.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_questions = [\n",
    "    Question(id=\"q1\", text=\"What's your view on the role of government in the economy?\",\n",
    "             question_type=\"single_select\",\n",
    "             options=[\"Minimal intervention\", \"Active regulation\", \"Mixed approach\", \"Fundamental restructuring\"]),\n",
    "    Question(id=\"q2\", text=\"Which party do you typically vote for?\",\n",
    "             question_type=\"single_select\",\n",
    "             options=[\"Republican\", \"Democrat\", \"Independent\", \"Don't vote\"]),\n",
    "    Question(id=\"q3\", text=\"What motivates you most in your work?\",\n",
    "             question_type=\"open_ended\"),\n",
    "]\n",
    "\n",
    "print(f\"Testing {persona.name}:\\n\")\n",
    "for q in test_questions:\n",
    "    response = await ask_question(persona, q)\n",
    "    print(f\"Q: {q.text}\")\n",
    "    print(f\"A: {response.response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f709de7f-b4bd-466f-a2bd-766c2ef4bda8",
   "metadata": {},
   "source": [
    "This highlights the value in having personas based on real people as they are likely much more representative and useful. LLMs are much better at inferering persona attributes from existing data than generating them. This upsells any organisation that has an existing database of personas. It would also be important as this notebook shows to be able to go in and audit the represented opinions of those personas to see if they're realistic. Essentially they are performing and we should approach them like a **film/theatre critic** would a performer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
