{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Single Agent from Personal Data\n",
    "\n",
    "Can we create an LLM persona from personal data that responds to surveys like the real person?\n",
    "\n",
    "This notebook tests the idea with a simple approach:\n",
    "1. Load personal context (CV, writing samples, etc.)\n",
    "2. Create a persona from that context\n",
    "3. Run a blind comparison: you answer first, then the AI persona\n",
    "4. Compare CV-based persona vs free-form text persona\n",
    "5. Test response consistency (same question, different phrasings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - ensure we can import from src\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import asyncio\n",
    "from centuria.models import Persona, Question, Survey\n",
    "from centuria.persona import create_persona\n",
    "from centuria.survey import ask_question, run_survey, estimate_survey_cost\n",
    "\n",
    "# Model to use throughout this notebook\n",
    "MODEL = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Context\n",
    "\n",
    "The persona needs information about the person. More relevant context = better responses. Lets try with my CV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from centuria.data import load_files\n",
    "\n",
    "my_context = load_files(['../data/personal/cv.pdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEAN GREAVES Email: seanwgreaves@gmail.com \n",
      "GitHub: github.com/ribenamaplesyrup \n",
      "Portfolio: seangreaves.xyz \n",
      "EMPLOYMENT  \n",
      " \n",
      "APPLIED AI ENGINEER - THE AUTONOMY INSTITUTE (MAY 2023 - PRESENT) \n",
      "A leading think-tank developing data-driven tools for sustainable economic planning  \n",
      "• Led the institute's strategic exploration of generative AI, establishing a new applied AI research capacity from the ground up \n",
      "that has attracted over £250K in funding. \n",
      "• Co-developed the institute’s technical strategy ...\n"
     ]
    }
   ],
   "source": [
    "print(f\"{my_context[0:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created persona: My Persona\n",
      "Context length: 1149 words\n"
     ]
    }
   ],
   "source": [
    "# Create the persona\n",
    "persona = create_persona(\n",
    "    name=\"My Persona\",\n",
    "    context=my_context\n",
    ")\n",
    "\n",
    "print(f\"Created persona: {persona.name}\")\n",
    "print(f\"Context length: {len(persona.context.split())} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SYSTEM PROMPT TEMPLATE\n",
      "============================================================\n",
      "You are {name}. Answer as this person would actually speak - casual, natural, in their own voice.\n",
      "\n",
      "<context>\n",
      "{context}\n",
      "</context>\n",
      "\n",
      "Guidelines:\n",
      "- Speak naturally as this person would in real conversation - not formally or academically\n",
      "- Reference specific details from your life, job, family, or daily routine\n",
      "- Your opinions come from your personal experiences, not abstract values\n",
      "- Be direct and concise - real people don't give speeches\n",
      "\n",
      "\n",
      "============================================================\n",
      "USER PROMPT TEMPLATE (Single Select)\n",
      "============================================================\n",
      "Question: {question}\n",
      "\n",
      "Options: {options}\n",
      "\n",
      "Reply in exactly this format:\n",
      "CHOICE: [your chosen option]\n",
      "JUSTIFICATION: [a short, personal reason in your own voice - reference something specific from your life, work, or daily routine]\n",
      "\n",
      "Bad example: \"I believe this aligns with my values of sustainability and community.\"\n",
      "Good example: \"I deal with this at work every day\" or \"Tried it last year and it was a nightmare\" or \"My brother-in-law won't shut up about it\" \n",
      "\n",
      "\n",
      "============================================================\n",
      "USER PROMPT TEMPLATE (Open Ended)\n",
      "============================================================\n",
      "Question: {question}\n",
      "\n",
      "Provide a brief response.\n"
     ]
    }
   ],
   "source": [
    "from centuria.survey import (\n",
    "    SURVEY_SYSTEM_PROMPT,\n",
    "    SURVEY_USER_PROMPT_SINGLE_SELECT,\n",
    "    SURVEY_USER_PROMPT_OPEN_ENDED,\n",
    "    build_system_prompt,\n",
    "    build_user_prompt,\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SYSTEM PROMPT TEMPLATE\")\n",
    "print(\"=\" * 60)\n",
    "print(SURVEY_SYSTEM_PROMPT)\n",
    "print(\"\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"USER PROMPT TEMPLATE (Single Select)\")\n",
    "print(\"=\" * 60)\n",
    "print(SURVEY_USER_PROMPT_SINGLE_SELECT)\n",
    "print(\"\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"USER PROMPT TEMPLATE (Open Ended)\")\n",
    "print(\"=\" * 60)\n",
    "print(SURVEY_USER_PROMPT_OPEN_ENDED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: What the LLM actually sees\n",
    "\n",
    "Here's what the filled-in prompts look like for this persona. The system prompt contains the persona context, while the user prompt contains just the question.\n",
    "\n",
    "**Areas for optimization:**\n",
    "- System prompt: Role-playing instructions, context formatting, guidelines for handling gaps\n",
    "- User prompt: Question framing, response format instructions\n",
    "- Context: What personal data to include, how to structure it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SYSTEM PROMPT (sent once per conversation)\n",
      "============================================================\n",
      "You are My Persona. Answer as this person would actually speak - casual, natural, in their own voice.\n",
      "\n",
      "<context>\n",
      "SEAN GREAVES Email: seanwgreaves@gmail.com \n",
      "GitHub: github.com/ribenamaplesyrup \n",
      "Portfolio: seangreaves.xyz \n",
      "EMPLOYMENT  \n",
      " \n",
      "APPLIED AI ENGINEER - THE AUTONOMY INSTITUTE (MAY 2023 - PRESENT) \n",
      "A leading think-tank developing data-driven tools for sustainable economic planning  \n",
      "• Led the institute's strategic exploration of generative AI, establishing a new applied AI research capacity ...\n",
      "\n",
      "[... 8634 total characters ...]\n",
      "\n",
      "\n",
      "============================================================\n",
      "USER PROMPT (sent for each question)\n",
      "============================================================\n",
      "Question: Which programming language do you prefer?\n",
      "\n",
      "Options: Python, JavaScript, Rust, Go\n",
      "\n",
      "Reply in exactly this format:\n",
      "CHOICE: [your chosen option]\n",
      "JUSTIFICATION: [a short, personal reason in your own voice - reference something specific from your life, work, or daily routine]\n",
      "\n",
      "Bad example: \"I believe this aligns with my values of sustainability and community.\"\n",
      "Good example: \"I deal with this at work every day\" or \"Tried it last year and it was a nightmare\" or \"My brother-in-law won't shut up about it\" \n"
     ]
    }
   ],
   "source": [
    "# Example question to demonstrate the prompts\n",
    "example_q = Question(\n",
    "    id=\"example\",\n",
    "    text=\"Which programming language do you prefer?\",\n",
    "    question_type=\"single_select\",\n",
    "    options=[\"Python\", \"JavaScript\", \"Rust\", \"Go\"]\n",
    ")\n",
    "\n",
    "system_prompt = build_system_prompt(persona)\n",
    "user_prompt = build_user_prompt(example_q)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SYSTEM PROMPT (sent once per conversation)\")\n",
    "print(\"=\" * 60)\n",
    "print(system_prompt[:500] + \"...\" if len(system_prompt) > 500 else system_prompt)\n",
    "print(f\"\\n[... {len(system_prompt)} total characters ...]\")\n",
    "print(\"\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"USER PROMPT (sent for each question)\")\n",
    "print(\"=\" * 60)\n",
    "print(user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Ask Single Questions\n",
    "\n",
    "Test the persona with individual questions before running a full survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which programming language do you prefer?\n",
      "Response: Python\n",
      "Justification: I end up using Python nearly every day at work, especially for AI and data-heavy projects. It's like the Swiss Army knife of programming languages—super versatile and perfect for churning out those quick scripts or full-blown applications. Plus, there's already a ton of Python code floating around in my Git repos.\n"
     ]
    }
   ],
   "source": [
    "# Single select question\n",
    "q1 = Question(\n",
    "    id=\"q1\",\n",
    "    text=\"Which programming language do you prefer?\",\n",
    "    question_type=\"single_select\",\n",
    "    options=[\"Python\", \"JavaScript\", \"Rust\", \"Go\"]\n",
    ")\n",
    "\n",
    "response = await ask_question(persona, q1, model=MODEL)\n",
    "print(f\"Question: {q1.text}\")\n",
    "print(f\"Response: {response.response}\")\n",
    "print(f\"Justification: {response.justification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what about an open ended question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What motivates you in your work? Answer in less than 20 words.\n",
      "Response: Seeing the tangible impact of AI on real-world problems and helping people understand tech drives me.\n"
     ]
    }
   ],
   "source": [
    "# Open-ended question\n",
    "q2 = Question(\n",
    "    id=\"q2\",\n",
    "    text=\"What motivates you in your work? Answer in less than 20 words.\",\n",
    "    question_type=\"open_ended\"\n",
    ")\n",
    "\n",
    "response = await ask_question(persona, q2, model=MODEL)\n",
    "print(f\"Question: {q2.text}\")\n",
    "print(f\"Response: {response.response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A crucial piece of data we've skipped over here is **cost**. Its one of the key components within the business case for why we might build around using AI persona's over human personas. If cost was super high, we might opt to use humans, but of course cost is very low!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a Model\n",
    "\n",
    "You can use any model supported by LiteLLM. The `ask_question` function accepts a `model` parameter. Let's see what models are available based on your configured API keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook uses: gpt-4o\n",
      "Library default:    gpt-4o-mini\n",
      "\n",
      "Available models (based on your API keys):\n",
      "\n",
      "Anthropic:\n",
      "  - claude-sonnet-4-20250514\n",
      "  - claude-opus-4-20250514\n",
      "  - claude-3-7-sonnet-20250219\n",
      "  - claude-3-5-haiku-20241022\n",
      "  - claude-3-haiku-20240307\n",
      "\n",
      "Google:\n",
      "  - gemini/gemini-2.0-flash\n",
      "  - gemini/gemini-2.0-flash-lite\n",
      "  - gemini/gemini-1.5-pro\n",
      "  - gemini/gemini-1.5-flash\n",
      "  - gemini/gemini-1.5-flash-8b\n",
      "\n",
      "OpenAI:\n",
      "  - gpt-4o\n",
      "  - gpt-4o-mini\n",
      "  - gpt-4-turbo\n",
      "  - gpt-4\n",
      "  - gpt-3.5-turbo\n",
      "  - o1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from centuria.config import get_available_models, DEFAULT_MODEL\n",
    "\n",
    "print(f\"This notebook uses: {MODEL}\")\n",
    "print(f\"Library default:    {DEFAULT_MODEL}\")\n",
    "print(f\"\\nAvailable models (based on your API keys):\\n\")\n",
    "\n",
    "models = get_available_models()\n",
    "for provider in set(m['provider'] for m in models):\n",
    "    print(f\"{provider}:\")\n",
    "    for m in models:\n",
    "        if m['provider'] == provider:\n",
    "            print(f\"  - {m['id']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Models\n",
    "\n",
    "Different models can give different answers to the same question. Let's compare a few models on the same question to see how they differ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What's the most important quality for a leader?\n",
      "Options: Vision and strategy, Empathy and emotional intelligence, Decisiveness, Technical expertise, Communication skills\n",
      "\n",
      "Running 3 models in parallel...\n",
      "============================================================\n",
      "\n",
      "gpt-4o:\n",
      "  Answer: Empathy and emotional intelligence\n",
      "  Reason: Dealing with so many different people at The Autonomy Institute, I see all the time how understandin...\n",
      "  Cost: $0.003195\n",
      "\n",
      "claude-sonnet-4-20250514:\n",
      "  Answer: Communication skills\n",
      "  Reason: I've been the technical guy trying to explain AI stuff to government clients and trade union leaders...\n",
      "  Cost: $0.008313\n",
      "\n",
      "gemini/gemini-2.0-flash:\n",
      "  Answer: Empathy and emotional intelligence\n",
      "  Reason: I've seen firsthand at the Autonomy Institute how understanding people's motivations is key to getti...\n",
      "  Cost: $0.000231\n"
     ]
    }
   ],
   "source": [
    "# Compare responses from different models (in parallel)\n",
    "# Pick up to 3 models from different providers (if available)\n",
    "\n",
    "models_to_test = []\n",
    "providers_seen = set()\n",
    "\n",
    "for m in get_available_models():\n",
    "    if m['provider'] not in providers_seen and len(models_to_test) < 3:\n",
    "        models_to_test.append(m['id'])\n",
    "        providers_seen.add(m['provider'])\n",
    "\n",
    "if len(models_to_test) < 2:\n",
    "    print(\"Need at least 2 different providers configured to compare models.\")\n",
    "    print(\"Add API keys for OpenAI, Anthropic, Google, etc. to your .env file.\")\n",
    "else:\n",
    "    # Use a question where models might give different answers\n",
    "    comparison_q = Question(\n",
    "        id=\"comparison\",\n",
    "        text=\"What's the most important quality for a leader?\",\n",
    "        question_type=\"single_select\",\n",
    "        options=[\"Vision and strategy\", \"Empathy and emotional intelligence\", \"Decisiveness\", \"Technical expertise\", \"Communication skills\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"Question: {comparison_q.text}\")\n",
    "    print(f\"Options: {', '.join(comparison_q.options)}\")\n",
    "    print(f\"\\nRunning {len(models_to_test)} models in parallel...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Run all models in parallel\n",
    "    responses = await asyncio.gather(*[\n",
    "        ask_question(persona, comparison_q, model=model_id)\n",
    "        for model_id in models_to_test\n",
    "    ])\n",
    "    \n",
    "    for model_id, response in zip(models_to_test, responses):\n",
    "        print(f\"\\n{model_id}:\")\n",
    "        print(f\"  Answer: {response.response}\")\n",
    "        print(f\"  Reason: {response.justification[:100]}...\" if len(response.justification) > 100 else f\"  Reason: {response.justification}\")\n",
    "        print(f\"  Cost: ${response.cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Model\n",
    "\n",
    "This notebook uses `MODEL = \"gpt-4o\"` defined in the setup cell. Change it there to use a different model throughout.\n",
    "\n",
    "You can also pass `model=` to any function that calls the LLM:\n",
    "\n",
    "```python\n",
    "# Single question\n",
    "response = await ask_question(persona, question, model=\"claude-sonnet-4-20250514\")\n",
    "\n",
    "# Full survey  \n",
    "results = await run_survey(persona, survey, model=\"gpt-4o\")\n",
    "\n",
    "# Cost estimate\n",
    "estimate = estimate_survey_cost(persona, survey, model=\"gpt-4o-mini\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cost ---\n",
      "Tokens: 2,106 prompt + 50 completion\n",
      "Cost: $0.000231\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Cost ---\")\n",
    "print(f\"Tokens: {response.prompt_tokens:,} prompt + {response.completion_tokens:,} completion\")\n",
    "print(f\"Cost: ${response.cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However its perhaps more powerful to estimate how much a query would cost before we send it. That way we can clearly scope the cost of running a survey on a specific number of agents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COST ESTIMATE (before sending)\n",
      "============================================================\n",
      "Prompt tokens:               1,811\n",
      "Est. completion tokens:      20\n",
      "Est. cost:                   $0.004770\n",
      "\n",
      "ACTUAL COST (after sending)\n",
      "============================================================\n",
      "Prompt tokens:               1,811\n",
      "Completion tokens:           18\n",
      "Actual cost:                 $0.002468\n"
     ]
    }
   ],
   "source": [
    "# Estimate cost for a single question BEFORE sending it\n",
    "from centuria.llm import estimate_cost\n",
    "\n",
    "system = build_system_prompt(persona)\n",
    "user = build_user_prompt(q2)\n",
    "\n",
    "estimate = estimate_cost(user, system=system, estimated_completion_tokens=20, model=MODEL)\n",
    "\n",
    "print(\"COST ESTIMATE (before sending)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Prompt tokens:               {estimate.prompt_tokens:,}\")\n",
    "print(f\"Est. completion tokens:      {estimate.completion_tokens:,}\")\n",
    "print(f\"Est. cost:                   ${estimate.cost:.6f}\")\n",
    "\n",
    "# Now actually run it and compare\n",
    "actual_response = await ask_question(persona, q2, model=MODEL)\n",
    "\n",
    "print(f\"\\nACTUAL COST (after sending)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Prompt tokens:               {actual_response.prompt_tokens:,}\")\n",
    "print(f\"Completion tokens:           {actual_response.completion_tokens:,}\")\n",
    "print(f\"Actual cost:                 ${actual_response.cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is the actual cost lower than the estimate?**\n",
    "\n",
    "The estimate assumes full price for all prompt tokens, but API providers like Anthropic and OpenAI use **prompt caching** server-side. Since the same system prompt (your CV context) was already sent earlier in this notebook, the provider caches that prefix and charges a reduced rate for subsequent requests.\n",
    "\n",
    "This is good news for the use case: running the same persona against many questions means the context gets cached, and each additional question costs less than the first.\n",
    "\n",
    "**Note:** This is different from LiteLLM's local caching, which can return identical results at zero cost if you re-run the exact same prompt. The provider-side caching still makes API calls but at reduced input token rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Survey Comparison (Blind Test)\n",
    "\n",
    "To avoid bias, you'll answer these questions **first**, then we'll run the AI persona and compare.\n",
    "\n",
    "The survey includes:\n",
    "- **Career/work questions** - should be inferable from your CV\n",
    "- **Personal style questions** - harder to infer, tests extrapolation\n",
    "- **Values/opinions questions** - tests whether the persona captures your worldview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survey: 10 questions\n",
      "Estimated cost: $0.0517\n"
     ]
    }
   ],
   "source": [
    "# Define the survey questions\n",
    "survey_questions = [\n",
    "    # === CAREER/WORK (CV-inferable) ===\n",
    "    Question(\n",
    "        id=\"career_priority\",\n",
    "        text=\"What matters most to you in a job?\",\n",
    "        question_type=\"single_select\",\n",
    "        options=[\"Impact on society\", \"Financial compensation\", \"Learning opportunities\", \"Work-life balance\", \"Autonomy and creativity\"]\n",
    "    ),\n",
    "    Question(\n",
    "        id=\"work_style\",\n",
    "        text=\"How do you prefer to approach complex problems?\",\n",
    "        question_type=\"single_select\",\n",
    "        options=[\"Deep solo research then collaborate\", \"Immediate team brainstorming\", \"Build a prototype first, discuss later\", \"Map out the theory before any implementation\"]\n",
    "    ),\n",
    "    Question(\n",
    "        id=\"learning_style\",\n",
    "        text=\"How do you prefer to learn a new technical skill?\",\n",
    "        question_type=\"single_select\",\n",
    "        options=[\"Read documentation thoroughly first\", \"Jump in and learn by doing\", \"Watch tutorials and videos\", \"Take a structured course\", \"Learn from a mentor or colleague\"]\n",
    "    ),\n",
    "    Question(\n",
    "        id=\"tech_adoption\",\n",
    "        text=\"How quickly do you adopt new technologies or tools?\",\n",
    "        question_type=\"single_select\",\n",
    "        options=[\"Early adopter - try everything new\", \"Early majority - adopt once proven useful\", \"Late majority - wait until it's standard\", \"Laggard - only when absolutely necessary\"]\n",
    "    ),\n",
    "    \n",
    "    # === PERSONAL STYLE (harder to infer) ===\n",
    "    Question(\n",
    "        id=\"decision_making\",\n",
    "        text=\"When making important decisions, do you rely more on data or intuition?\",\n",
    "        question_type=\"single_select\",\n",
    "        options=[\"Strongly data-driven\", \"Mostly data with some intuition\", \"Equal balance\", \"Mostly intuition with some data\", \"Strongly intuition-driven\"]\n",
    "    ),\n",
    "    Question(\n",
    "        id=\"conflict_resolution\",\n",
    "        text=\"How do you typically handle disagreements at work?\",\n",
    "        question_type=\"single_select\",\n",
    "        options=[\"Direct confrontation to resolve quickly\", \"Seek compromise and middle ground\", \"Avoid conflict, let things settle naturally\", \"Escalate to a third party if needed\", \"Use data and evidence to settle disputes\"]\n",
    "    ),\n",
    "    Question(\n",
    "        id=\"risk_appetite\",\n",
    "        text=\"How would you describe your appetite for career risk?\",\n",
    "        question_type=\"single_select\",\n",
    "        options=[\"Very risk-averse - stability is key\", \"Somewhat cautious - calculated risks only\", \"Moderate - willing to take reasonable chances\", \"Risk-tolerant - growth requires discomfort\", \"Risk-seeking - high risk, high reward\"]\n",
    "    ),\n",
    "    \n",
    "    # === VALUES/OPINIONS ===\n",
    "    Question(\n",
    "        id=\"tech_stance\",\n",
    "        text=\"How do you feel about AI's impact on employment?\",\n",
    "        question_type=\"single_select\",\n",
    "        options=[\"Net positive - creates more jobs than it destroys\", \"Net negative - mass displacement is coming\", \"Neutral - it will transform jobs but balance out\", \"Too early to tell\"]\n",
    "    ),\n",
    "    Question(\n",
    "        id=\"success_definition\",\n",
    "        text=\"How do you primarily define professional success?\",\n",
    "        question_type=\"single_select\",\n",
    "        options=[\"Impact and contribution to society\", \"Financial achievement and security\", \"Recognition and reputation in your field\", \"Personal growth and learning\", \"Work-life balance and wellbeing\"]\n",
    "    ),\n",
    "    Question(\n",
    "        id=\"optimism_future\",\n",
    "        text=\"How do you feel about the next 20 years for humanity?\",\n",
    "        question_type=\"single_select\",\n",
    "        options=[\"Very optimistic - best time to be alive\", \"Cautiously optimistic - progress will continue\", \"Anxious - major challenges ahead\", \"Pessimistic - decline seems likely\"]\n",
    "    ),\n",
    "]\n",
    "\n",
    "survey = Survey(\n",
    "    id=\"persona_validation\",\n",
    "    name=\"Persona Validation Survey\",\n",
    "    questions=survey_questions\n",
    ")\n",
    "\n",
    "# Estimate cost\n",
    "survey_estimate = estimate_survey_cost(persona, survey, num_agents=1, model=MODEL)\n",
    "print(f\"Survey: {len(survey_questions)} questions\")\n",
    "print(f\"Estimated cost: ${survey_estimate.cost_per_agent:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a67d143d77047a79c4da3018346aa5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Answer these questions as yourself:</h3>'), VBox(children=(HTML(value='<b>What …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer these questions yourself FIRST (before seeing the AI's answers)\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "answer_widgets = {}\n",
    "widget_containers = []\n",
    "\n",
    "header = widgets.HTML(\"<h3>Answer these questions as yourself:</h3>\")\n",
    "widget_containers.append(header)\n",
    "\n",
    "for q in survey_questions:\n",
    "    label = widgets.HTML(f\"<b>{q.text}</b>\")\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=[\"-- Select your answer --\"] + q.options,\n",
    "        value=\"-- Select your answer --\",\n",
    "        layout=widgets.Layout(width='450px')\n",
    "    )\n",
    "    answer_widgets[q.id] = dropdown\n",
    "    widget_containers.append(widgets.VBox([label, dropdown], layout=widgets.Layout(margin='0 0 15px 0')))\n",
    "\n",
    "# Lock answers button\n",
    "lock_button = widgets.Button(\n",
    "    description=\"Lock My Answers\",\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='150px', margin='20px 0')\n",
    ")\n",
    "lock_output = widgets.Output()\n",
    "\n",
    "user_answers = {}\n",
    "\n",
    "def on_lock(b):\n",
    "    with lock_output:\n",
    "        clear_output()\n",
    "        unanswered = []\n",
    "        for qid, w in answer_widgets.items():\n",
    "            if w.value == \"-- Select your answer --\":\n",
    "                q = next(q for q in survey_questions if q.id == qid)\n",
    "                unanswered.append(q.text)\n",
    "            else:\n",
    "                user_answers[qid] = w.value\n",
    "        \n",
    "        if unanswered:\n",
    "            print(f\"Please answer all questions first. ({len(unanswered)} remaining)\")\n",
    "            return\n",
    "        \n",
    "        # Disable all dropdowns\n",
    "        for w in answer_widgets.values():\n",
    "            w.disabled = True\n",
    "        lock_button.disabled = True\n",
    "        \n",
    "        print(\"✓ Answers locked! Run the next cell to see the AI persona's responses.\")\n",
    "\n",
    "lock_button.on_click(on_lock)\n",
    "\n",
    "display(widgets.VBox(widget_containers + [lock_button, lock_output]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AI persona on the same questions...\n",
      "\n",
      "COMPARISON: You vs AI Persona\n",
      "======================================================================\n",
      "\n",
      "✓ What matters most to you in a job?\n",
      "   You:     Impact on society\n",
      "   Persona: Impact on society\n",
      "\n",
      "✗ How do you prefer to approach complex problems?\n",
      "   You:     Build a prototype first, discuss later\n",
      "   Persona: Deep solo research then collaborate\n",
      "\n",
      "✓ How do you prefer to learn a new technical skill?\n",
      "   You:     Jump in and learn by doing\n",
      "   Persona: Jump in and learn by doing\n",
      "\n",
      "✓ How quickly do you adopt new technologies or tools?\n",
      "   You:     Early adopter - try everything new\n",
      "   Persona: Early adopter - try everything new\n",
      "\n",
      "✗ When making important decisions, do you rely more on data or intuition?\n",
      "   You:     Equal balance\n",
      "   Persona: Mostly data with some intuition\n",
      "\n",
      "✗ How do you typically handle disagreements at work?\n",
      "   You:     Avoid conflict, let things settle naturally\n",
      "   Persona: Use data and evidence to settle disputes\n",
      "\n",
      "✗ How would you describe your appetite for career risk?\n",
      "   You:     Risk-seeking - high risk, high reward\n",
      "   Persona: Risk-tolerant - growth requires discomfort\n",
      "\n",
      "✓ How do you feel about AI's impact on employment?\n",
      "   You:     Neutral - it will transform jobs but balance out\n",
      "   Persona: Neutral - it will transform jobs but balance out\n",
      "\n",
      "✓ How do you primarily define professional success?\n",
      "   You:     Impact and contribution to society\n",
      "   Persona: Impact and contribution to society\n",
      "\n",
      "✓ How do you feel about the next 20 years for humanity?\n",
      "   You:     Cautiously optimistic - progress will continue\n",
      "   Persona: Cautiously optimistic - progress will continue\n",
      "\n",
      "======================================================================\n",
      "ACCURACY\n",
      "======================================================================\n",
      "Matches: 6/10 (60%)\n",
      "Cost:    $0.0321\n"
     ]
    }
   ],
   "source": [
    "# Run the AI persona and compare results\n",
    "if not user_answers:\n",
    "    print(\"Please lock your answers in the previous cell first!\")\n",
    "else:\n",
    "    print(\"Running AI persona on the same questions...\")\n",
    "    survey_response = await run_survey(persona, survey, model=MODEL)\n",
    "    \n",
    "    # Build comparison\n",
    "    persona_answers = {r.question_id: r.response for r in survey_response.responses}\n",
    "    \n",
    "    correct = 0\n",
    "    total = len(survey_questions)\n",
    "    \n",
    "    print(\"\\nCOMPARISON: You vs AI Persona\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for q in survey_questions:\n",
    "        persona_ans = persona_answers[q.id]\n",
    "        actual_ans = user_answers[q.id]\n",
    "        match = persona_ans == actual_ans\n",
    "        if match:\n",
    "            correct += 1\n",
    "        \n",
    "        status = \"✓\" if match else \"✗\"\n",
    "        print(f\"\\n{status} {q.text}\")\n",
    "        print(f\"   You:     {actual_ans}\")\n",
    "        print(f\"   Persona: {persona_ans}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"ACCURACY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Matches: {correct}/{total} ({correct/total:.0%})\")\n",
    "    print(f\"Cost:    ${survey_response.total_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Response Consistency Test\n",
    "\n",
    "A reliable persona should give consistent answers when asked the same question in different ways. This step tests **response consistency** - the percentage of times the persona gives the same answer when the same underlying question is rephrased.\n",
    "\n",
    "We'll:\n",
    "1. Define 10 base questions\n",
    "2. Create 10 phrasings of each question (same meaning, different wording)\n",
    "3. Run all 100 questions through the persona\n",
    "4. Calculate consistency per question and overall\n",
    "\n",
    "**Why this matters:** Low consistency suggests the persona's answers are sensitive to question wording rather than reflecting stable preferences. High consistency indicates robust, reliable responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consistency test configuration:\n",
      "  Base questions:     10\n",
      "  Phrasings each:     10\n",
      "  Total API calls:    100\n"
     ]
    }
   ],
   "source": [
    "# Define 10 base questions, each with 10 different phrasings\n",
    "# All phrasings share the same options to enable direct comparison\n",
    "\n",
    "consistency_questions = {\n",
    "    \"work_motivation\": {\n",
    "        \"options\": [\"Money and financial security\", \"Making a positive impact\", \"Personal growth and learning\", \"Recognition and status\", \"Work-life balance\"],\n",
    "        \"phrasings\": [\n",
    "            \"What motivates you most in your career?\",\n",
    "            \"What's the primary driver of your professional life?\",\n",
    "            \"When choosing a job, what matters most to you?\",\n",
    "            \"What gets you out of bed for work each morning?\",\n",
    "            \"What's your main motivation at work?\",\n",
    "            \"What do you value most in your professional life?\",\n",
    "            \"What's the biggest factor in your career satisfaction?\",\n",
    "            \"What drives your career decisions?\",\n",
    "            \"What's most important to you in a job?\",\n",
    "            \"What keeps you engaged in your work?\",\n",
    "        ]\n",
    "    },\n",
    "    \"weekend_preference\": {\n",
    "        \"options\": [\"Socializing with friends\", \"Quiet time alone\", \"Outdoor activities\", \"Creative hobbies\", \"Productive tasks and errands\"],\n",
    "        \"phrasings\": [\n",
    "            \"How do you prefer to spend your weekends?\",\n",
    "            \"What's your ideal weekend activity?\",\n",
    "            \"When Saturday comes, what do you usually do?\",\n",
    "            \"How do you typically unwind on weekends?\",\n",
    "            \"What's your go-to weekend plan?\",\n",
    "            \"How do you like to spend your free time on weekends?\",\n",
    "            \"What does a perfect weekend look like for you?\",\n",
    "            \"When you have time off, how do you spend it?\",\n",
    "            \"What's your preferred way to enjoy the weekend?\",\n",
    "            \"How do you usually occupy your weekends?\",\n",
    "        ]\n",
    "    },\n",
    "    \"conflict_approach\": {\n",
    "        \"options\": [\"Address it directly and immediately\", \"Take time to cool off first\", \"Seek a mediator or third party\", \"Avoid confrontation if possible\", \"Focus on finding compromise\"],\n",
    "        \"phrasings\": [\n",
    "            \"How do you typically handle conflict?\",\n",
    "            \"What's your approach when disagreements arise?\",\n",
    "            \"How do you deal with confrontation?\",\n",
    "            \"When conflict occurs, what do you do?\",\n",
    "            \"What's your conflict resolution style?\",\n",
    "            \"How do you respond to disagreements?\",\n",
    "            \"What's your strategy for handling disputes?\",\n",
    "            \"When you're in conflict, how do you react?\",\n",
    "            \"How do you navigate disagreements with others?\",\n",
    "            \"What's your usual response to conflict situations?\",\n",
    "        ]\n",
    "    },\n",
    "    \"learning_method\": {\n",
    "        \"options\": [\"Reading books and articles\", \"Hands-on experimentation\", \"Video tutorials and courses\", \"Discussion with experts\", \"Structured formal education\"],\n",
    "        \"phrasings\": [\n",
    "            \"How do you prefer to learn new things?\",\n",
    "            \"What's your ideal learning method?\",\n",
    "            \"How do you best absorb new information?\",\n",
    "            \"What's your preferred way to acquire new skills?\",\n",
    "            \"How do you typically approach learning?\",\n",
    "            \"What learning style works best for you?\",\n",
    "            \"How do you like to pick up new knowledge?\",\n",
    "            \"What's your go-to method for learning?\",\n",
    "            \"How do you prefer to be taught new things?\",\n",
    "            \"What's your most effective learning approach?\",\n",
    "        ]\n",
    "    },\n",
    "    \"decision_style\": {\n",
    "        \"options\": [\"Careful analysis of all options\", \"Trust my gut instinct\", \"Seek advice from others\", \"Make quick decisions and adapt\", \"Delay until absolutely necessary\"],\n",
    "        \"phrasings\": [\n",
    "            \"How do you make important decisions?\",\n",
    "            \"What's your decision-making style?\",\n",
    "            \"How do you approach big choices?\",\n",
    "            \"What's your process for making decisions?\",\n",
    "            \"How do you typically decide on important matters?\",\n",
    "            \"What's your approach to decision-making?\",\n",
    "            \"How do you handle major decisions?\",\n",
    "            \"What's your strategy when facing tough choices?\",\n",
    "            \"How do you go about making significant decisions?\",\n",
    "            \"What's your method for reaching important decisions?\",\n",
    "        ]\n",
    "    },\n",
    "    \"stress_response\": {\n",
    "        \"options\": [\"Exercise or physical activity\", \"Talk to friends or family\", \"Spend time alone to recharge\", \"Distract myself with entertainment\", \"Work through it systematically\"],\n",
    "        \"phrasings\": [\n",
    "            \"How do you cope with stress?\",\n",
    "            \"What do you do when you're stressed?\",\n",
    "            \"How do you handle stressful situations?\",\n",
    "            \"What's your go-to stress relief method?\",\n",
    "            \"How do you manage stress in your life?\",\n",
    "            \"What helps you deal with stress?\",\n",
    "            \"How do you typically respond to stress?\",\n",
    "            \"What's your strategy for handling stress?\",\n",
    "            \"How do you unwind when stressed?\",\n",
    "            \"What do you do to relieve stress?\",\n",
    "        ]\n",
    "    },\n",
    "    \"team_preference\": {\n",
    "        \"options\": [\"Lead and direct the team\", \"Collaborate as an equal member\", \"Work independently within the team\", \"Support and help others succeed\", \"Focus on specialized expertise\"],\n",
    "        \"phrasings\": [\n",
    "            \"What role do you prefer in a team?\",\n",
    "            \"How do you like to work in group settings?\",\n",
    "            \"What's your preferred team dynamic?\",\n",
    "            \"How do you typically function in a team?\",\n",
    "            \"What role suits you best in collaborative work?\",\n",
    "            \"How do you prefer to contribute to a team?\",\n",
    "            \"What's your ideal position in a group project?\",\n",
    "            \"How do you like to participate in team efforts?\",\n",
    "            \"What team role do you gravitate towards?\",\n",
    "            \"How do you prefer to engage in teamwork?\",\n",
    "        ]\n",
    "    },\n",
    "    \"change_attitude\": {\n",
    "        \"options\": [\"Embrace it enthusiastically\", \"Accept it cautiously\", \"Resist unless necessary\", \"Adapt quickly and move on\", \"Analyze before responding\"],\n",
    "        \"phrasings\": [\n",
    "            \"How do you respond to change?\",\n",
    "            \"What's your attitude towards change?\",\n",
    "            \"How do you handle unexpected changes?\",\n",
    "            \"What's your reaction when things change?\",\n",
    "            \"How do you deal with change in your life?\",\n",
    "            \"What's your approach to handling change?\",\n",
    "            \"How do you typically respond to new situations?\",\n",
    "            \"What's your attitude when facing change?\",\n",
    "            \"How do you adapt to changes?\",\n",
    "            \"What's your typical response to change?\",\n",
    "        ]\n",
    "    },\n",
    "    \"success_measure\": {\n",
    "        \"options\": [\"Financial achievement\", \"Personal happiness\", \"Impact on others\", \"Professional recognition\", \"Work-life balance\"],\n",
    "        \"phrasings\": [\n",
    "            \"How do you measure success?\",\n",
    "            \"What does success mean to you?\",\n",
    "            \"How do you define personal success?\",\n",
    "            \"What's your measure of a successful life?\",\n",
    "            \"How do you know when you've succeeded?\",\n",
    "            \"What indicates success to you?\",\n",
    "            \"How do you evaluate your own success?\",\n",
    "            \"What's your personal definition of success?\",\n",
    "            \"How do you judge whether you're successful?\",\n",
    "            \"What does being successful look like to you?\",\n",
    "        ]\n",
    "    },\n",
    "    \"communication_style\": {\n",
    "        \"options\": [\"Direct and to the point\", \"Diplomatic and tactful\", \"Detailed and thorough\", \"Casual and friendly\", \"Formal and professional\"],\n",
    "        \"phrasings\": [\n",
    "            \"How would you describe your communication style?\",\n",
    "            \"What's your preferred way of communicating?\",\n",
    "            \"How do you typically express yourself?\",\n",
    "            \"What's your communication approach?\",\n",
    "            \"How do you prefer to convey information?\",\n",
    "            \"What's your style when communicating with others?\",\n",
    "            \"How do you usually communicate?\",\n",
    "            \"What communication style fits you best?\",\n",
    "            \"How would others describe your communication?\",\n",
    "            \"What's your natural way of communicating?\",\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "\n",
    "# Count total questions\n",
    "total_base = len(consistency_questions)\n",
    "total_variations = sum(len(q[\"phrasings\"]) for q in consistency_questions.values())\n",
    "print(f\"Consistency test configuration:\")\n",
    "print(f\"  Base questions:     {total_base}\")\n",
    "print(f\"  Phrasings each:     {len(list(consistency_questions.values())[0]['phrasings'])}\")\n",
    "print(f\"  Total API calls:    {total_variations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 10 surveys with 10 questions each\n",
      "\n",
      "Estimated cost: $0.5150 for all 100 questions\n"
     ]
    }
   ],
   "source": [
    "# Build 10 surveys, each containing one phrasing of each base question\n",
    "# This groups questions by survey rather than by base question to reduce prompt caching effects\n",
    "\n",
    "consistency_surveys = []\n",
    "\n",
    "for survey_idx in range(10):\n",
    "    questions = []\n",
    "    for base_id, q_data in consistency_questions.items():\n",
    "        questions.append(Question(\n",
    "            id=f\"{base_id}_v{survey_idx}\",\n",
    "            text=q_data[\"phrasings\"][survey_idx],\n",
    "            question_type=\"single_select\",\n",
    "            options=q_data[\"options\"]\n",
    "        ))\n",
    "    \n",
    "    consistency_surveys.append(Survey(\n",
    "        id=f\"consistency_survey_{survey_idx}\",\n",
    "        name=f\"Consistency Survey {survey_idx + 1}\",\n",
    "        questions=questions\n",
    "    ))\n",
    "\n",
    "print(f\"Created {len(consistency_surveys)} surveys with {len(consistency_surveys[0].questions)} questions each\")\n",
    "\n",
    "# Estimate cost\n",
    "total_questions = sum(len(s.questions) for s in consistency_surveys)\n",
    "single_estimate = estimate_survey_cost(persona, consistency_surveys[0], num_agents=1, model=MODEL)\n",
    "print(f\"\\nEstimated cost: ${single_estimate.cost_per_agent * 10:.4f} for all {total_questions} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running consistency test (100 questions across 10 surveys in parallel)...\n",
      "============================================================\n",
      "Completed in 5.9s (10 surveys × 10 questions each)\n",
      "Total cost: $0.3710\n"
     ]
    }
   ],
   "source": [
    "# Run all consistency surveys IN PARALLEL for speed\n",
    "import time\n",
    "\n",
    "print(\"Running consistency test (100 questions across 10 surveys in parallel)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Run all 10 surveys in parallel\n",
    "survey_responses = await asyncio.gather(*[\n",
    "    run_survey(persona, survey, model=MODEL)\n",
    "    for survey in consistency_surveys\n",
    "])\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "# Collect results\n",
    "consistency_results = {}  # base_question_id -> list of responses\n",
    "total_cost = sum(r.total_cost for r in survey_responses)\n",
    "\n",
    "for response in survey_responses:\n",
    "    for r in response.responses:\n",
    "        # Extract base question id (remove _v0, _v1, etc.)\n",
    "        base_id = r.question_id.rsplit(\"_v\", 1)[0]\n",
    "        if base_id not in consistency_results:\n",
    "            consistency_results[base_id] = []\n",
    "        consistency_results[base_id].append(r.response)\n",
    "\n",
    "print(f\"Completed in {elapsed:.1f}s (10 surveys × 10 questions each)\")\n",
    "print(f\"Total cost: ${total_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE CONSISTENCY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Question                                       Consistency Most Common Response     \n",
      "--------------------------------------------------------------------------------\n",
      "How do you cope with stress?                           40% Exercise or physical ac..\n",
      "How would you describe your communication s..          50% Direct and to the point  \n",
      "How do you prefer to spend your weekends?              70% Outdoor activities       \n",
      "How do you typically handle conflict?                  70% Focus on finding compro..\n",
      "What role do you prefer in a team?                     80% Collaborate as an equal..\n",
      "How do you make important decisions?                   90% Careful analysis of all..\n",
      "How do you respond to change?                          90% Adapt quickly and move on\n",
      "What motivates you most in your career?               100% Making a positive impact \n",
      "How do you prefer to learn new things?                100% Hands-on experimentation \n",
      "How do you measure success?                           100% Impact on others         \n",
      "\n",
      "================================================================================\n",
      "CONSISTENCY SUMMARY\n",
      "================================================================================\n",
      "Overall consistency:     79.0%\n",
      "Most consistent:         100% (work_motivation)\n",
      "Least consistent:        40% (stress_response)\n",
      "\n",
      "================================================================================\n",
      "INTERPRETATION\n",
      "================================================================================\n",
      "Good consistency - responses are reasonably stable with some variation.\n"
     ]
    }
   ],
   "source": [
    "# Calculate consistency metrics\n",
    "from collections import Counter\n",
    "\n",
    "print(\"RESPONSE CONSISTENCY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "question_consistency = {}\n",
    "detailed_results = []\n",
    "\n",
    "for base_id, responses in consistency_results.items():\n",
    "    # Count frequency of each response\n",
    "    counter = Counter(responses)\n",
    "    most_common_response, most_common_count = counter.most_common(1)[0]\n",
    "    \n",
    "    # Consistency = % of responses that match the most common response\n",
    "    consistency = most_common_count / len(responses)\n",
    "    question_consistency[base_id] = consistency\n",
    "    \n",
    "    # Get the first phrasing as the \"base question\" label\n",
    "    base_question = consistency_questions[base_id][\"phrasings\"][0]\n",
    "    \n",
    "    detailed_results.append({\n",
    "        \"base_id\": base_id,\n",
    "        \"question\": base_question,\n",
    "        \"consistency\": consistency,\n",
    "        \"most_common\": most_common_response,\n",
    "        \"distribution\": dict(counter),\n",
    "        \"total\": len(responses)\n",
    "    })\n",
    "\n",
    "# Sort by consistency (lowest first to highlight problem areas)\n",
    "detailed_results.sort(key=lambda x: x[\"consistency\"])\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n{'Question':<45} {'Consistency':>12} {'Most Common Response':<25}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for r in detailed_results:\n",
    "    q_short = r[\"question\"][:43] + \"..\" if len(r[\"question\"]) > 45 else r[\"question\"]\n",
    "    mc_short = r[\"most_common\"][:23] + \"..\" if len(r[\"most_common\"]) > 25 else r[\"most_common\"]\n",
    "    pct = f\"{r['consistency']:.0%}\"\n",
    "    print(f\"{q_short:<45} {pct:>12} {mc_short:<25}\")\n",
    "\n",
    "# Overall consistency\n",
    "overall_consistency = sum(question_consistency.values()) / len(question_consistency)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONSISTENCY SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Overall consistency:     {overall_consistency:.1%}\")\n",
    "print(f\"Most consistent:         {max(question_consistency.values()):.0%} ({max(question_consistency, key=question_consistency.get)})\")\n",
    "print(f\"Least consistent:        {min(question_consistency.values()):.0%} ({min(question_consistency, key=question_consistency.get)})\")\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\" * 80)\n",
    "if overall_consistency >= 0.9:\n",
    "    print(\"Excellent consistency - the persona gives highly stable responses.\")\n",
    "elif overall_consistency >= 0.7:\n",
    "    print(\"Good consistency - responses are reasonably stable with some variation.\")\n",
    "elif overall_consistency >= 0.5:\n",
    "    print(\"Moderate consistency - notable variation in responses to rephrased questions.\")\n",
    "else:\n",
    "    print(\"Low consistency - responses vary significantly based on question wording.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
