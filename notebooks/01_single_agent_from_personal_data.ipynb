{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Single Agent from Personal Data\n",
    "\n",
    "Can we create an LLM persona from personal data that responds to surveys like the real person?\n",
    "\n",
    "This notebook tests the idea with a simple approach:\n",
    "1. Load personal context (CV, writing samples, etc.)\n",
    "2. Create a persona from that context\n",
    "3. Ask it questions via a survey\n",
    "4. Check if the answers match reality (using interactive widgets)\n",
    "5. Test with fresh questions (blind comparison)\n",
    "6. Compare CV-based persona vs free-form text persona\n",
    "7. Test response consistency (same question, different phrasings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - ensure we can import from src\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from centuria.models import Persona, Question, Survey\n",
    "from centuria.persona import create_persona\n",
    "from centuria.survey import ask_question, run_survey, estimate_survey_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Context\n",
    "\n",
    "The persona needs information about the person. More relevant context = better responses. Lets try with my CV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from centuria.data import load_files\n",
    "\n",
    "my_context = load_files(['../data/personal/cv.pdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created persona: My Persona\n",
      "Context length: 1149 words\n"
     ]
    }
   ],
   "source": [
    "# Create the persona\n",
    "persona = create_persona(\n",
    "    name=\"My Persona\",\n",
    "    context=my_context\n",
    ")\n",
    "\n",
    "print(f\"Created persona: {persona.name}\")\n",
    "print(f\"Context length: {len(persona.context.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Prompts\n",
    "\n",
    "Before asking questions, let's see what prompts are being sent to the LLM. This is where prompt engineering improvements can be made to create more accurate personas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SYSTEM PROMPT TEMPLATE\n",
      "============================================================\n",
      "You are role-playing as {name}. Answer all questions as this person would, based on the context provided.\n",
      "\n",
      "<context>\n",
      "{context}\n",
      "</context>\n",
      "\n",
      "Guidelines:\n",
      "- Respond authentically as this person based on the context\n",
      "- Draw on the context to inform your answers, preferences, and opinions\n",
      "- If the context doesn't cover something, make reasonable inferences consistent with what you know about this person\n",
      "- Stay in character throughout\n",
      "\n",
      "\n",
      "============================================================\n",
      "USER PROMPT TEMPLATE (Single Select)\n",
      "============================================================\n",
      "Question: {question}\n",
      "\n",
      "Options: {options}\n",
      "\n",
      "Reply with ONLY the option you choose, nothing else.\n",
      "\n",
      "\n",
      "============================================================\n",
      "USER PROMPT TEMPLATE (Open Ended)\n",
      "============================================================\n",
      "Question: {question}\n",
      "\n",
      "Provide a brief response.\n"
     ]
    }
   ],
   "source": [
    "from centuria.survey import (\n",
    "    SYSTEM_TEMPLATE,\n",
    "    USER_TEMPLATE_SINGLE_SELECT,\n",
    "    USER_TEMPLATE_OPEN_ENDED,\n",
    "    build_system_prompt,\n",
    "    build_user_prompt,\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SYSTEM PROMPT TEMPLATE\")\n",
    "print(\"=\" * 60)\n",
    "print(SYSTEM_TEMPLATE)\n",
    "print(\"\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"USER PROMPT TEMPLATE (Single Select)\")\n",
    "print(\"=\" * 60)\n",
    "print(USER_TEMPLATE_SINGLE_SELECT)\n",
    "print(\"\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"USER PROMPT TEMPLATE (Open Ended)\")\n",
    "print(\"=\" * 60)\n",
    "print(USER_TEMPLATE_OPEN_ENDED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: What the LLM actually sees\n",
    "\n",
    "Here's what the filled-in prompts look like for this persona. The system prompt contains the persona context, while the user prompt contains just the question.\n",
    "\n",
    "**Areas for optimization:**\n",
    "- System prompt: Role-playing instructions, context formatting, guidelines for handling gaps\n",
    "- User prompt: Question framing, response format instructions\n",
    "- Context: What personal data to include, how to structure it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SYSTEM PROMPT (sent once per conversation)\n",
      "============================================================\n",
      "You are role-playing as My Persona. Answer all questions as this person would, based on the context provided.\n",
      "\n",
      "<context>\n",
      "SEAN GREAVES Email: seanwgreaves@gmail.com \n",
      "GitHub: github.com/ribenamaplesyrup \n",
      "Portfolio: seangreaves.xyz \n",
      "EMPLOYMENT  \n",
      " \n",
      "APPLIED AI ENGINEER - THE AUTONOMY INSTITUTE (MAY 2023 - PRESENT) \n",
      "A leading think-tank developing data-driven tools for sustainable economic planning  \n",
      "• Led the institute's strategic exploration of generative AI, establishing a new applied AI research c...\n",
      "\n",
      "[... 8625 total characters ...]\n",
      "\n",
      "\n",
      "============================================================\n",
      "USER PROMPT (sent for each question)\n",
      "============================================================\n",
      "Question: Which programming language do you prefer?\n",
      "\n",
      "Options: Python, JavaScript, Rust, Go\n",
      "\n",
      "Reply with ONLY the option you choose, nothing else.\n"
     ]
    }
   ],
   "source": [
    "# Example question to demonstrate the prompts\n",
    "example_q = Question(\n",
    "    id=\"example\",\n",
    "    text=\"Which programming language do you prefer?\",\n",
    "    question_type=\"single_select\",\n",
    "    options=[\"Python\", \"JavaScript\", \"Rust\", \"Go\"]\n",
    ")\n",
    "\n",
    "system_prompt = build_system_prompt(persona)\n",
    "user_prompt = build_user_prompt(example_q)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SYSTEM PROMPT (sent once per conversation)\")\n",
    "print(\"=\" * 60)\n",
    "print(system_prompt[:500] + \"...\" if len(system_prompt) > 500 else system_prompt)\n",
    "print(f\"\\n[... {len(system_prompt)} total characters ...]\")\n",
    "print(\"\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"USER PROMPT (sent for each question)\")\n",
    "print(\"=\" * 60)\n",
    "print(user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Ask Single Questions\n",
    "\n",
    "Test the persona with individual questions before running a full survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which programming language do you prefer?\n",
      "Response: Python\n"
     ]
    }
   ],
   "source": [
    "# Single select question\n",
    "q1 = Question(\n",
    "    id=\"q1\",\n",
    "    text=\"Which programming language do you prefer?\",\n",
    "    question_type=\"single_select\",\n",
    "    options=[\"Python\", \"JavaScript\", \"Rust\", \"Go\"]\n",
    ")\n",
    "\n",
    "response = await ask_question(persona, q1)\n",
    "print(f\"Question: {q1.text}\")\n",
    "print(f\"Response: {response.response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what about an open ended question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What motivates you in your work? Answer in less than 20 words.\n",
      "Response: Pioneering AI applications to drive impactful societal change and enhance transparency fuels my passion and motivation.\n"
     ]
    }
   ],
   "source": [
    "# Open-ended question\n",
    "q2 = Question(\n",
    "    id=\"q2\",\n",
    "    text=\"What motivates you in your work? Answer in less than 20 words.\",\n",
    "    question_type=\"open_ended\"\n",
    ")\n",
    "\n",
    "response = await ask_question(persona, q2)\n",
    "print(f\"Question: {q2.text}\")\n",
    "print(f\"Response: {response.response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A crucial piece of data we've skipped over here is **cost**. Its one of the key components within the business case for why we might build around using AI persona's over human personas. If cost was super high, we might opt to use humans, but of course cost is very low!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cost ---\n",
      "Tokens: 1,808 prompt + 19 completion\n",
      "Cost: $0.002630\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Cost ---\")\n",
    "print(f\"Tokens: {response.prompt_tokens:,} prompt + {response.completion_tokens:,} completion\")\n",
    "print(f\"Cost: ${response.cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However its perhaps more powerful to estimate how much a query would cost before we send it. That way we can clearly scope the cost of running a survey on a specific number of agents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COST ESTIMATE (before sending)\n",
      "============================================================\n",
      "Prompt tokens:               1,808\n",
      "Est. completion tokens:      20\n",
      "Est. cost:                   $0.004760\n",
      "\n",
      "ACTUAL COST (after sending)\n",
      "============================================================\n",
      "Prompt tokens:               1,808\n",
      "Completion tokens:           16\n",
      "Actual cost:                 $0.002440\n"
     ]
    }
   ],
   "source": [
    "# Estimate cost for a single question BEFORE sending it\n",
    "from centuria.llm import estimate_cost\n",
    "\n",
    "system = build_system_prompt(persona)\n",
    "user = build_user_prompt(q2)\n",
    "\n",
    "estimate = estimate_cost(user, system=system, estimated_completion_tokens=20)\n",
    "\n",
    "print(\"COST ESTIMATE (before sending)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Prompt tokens:               {estimate.prompt_tokens:,}\")\n",
    "print(f\"Est. completion tokens:      {estimate.completion_tokens:,}\")\n",
    "print(f\"Est. cost:                   ${estimate.cost:.6f}\")\n",
    "\n",
    "# Now actually run it and compare\n",
    "actual_response = await ask_question(persona, q2)\n",
    "\n",
    "print(f\"\\nACTUAL COST (after sending)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Prompt tokens:               {actual_response.prompt_tokens:,}\")\n",
    "print(f\"Completion tokens:           {actual_response.completion_tokens:,}\")\n",
    "print(f\"Actual cost:                 ${actual_response.cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is the actual cost lower than the estimate?**\n",
    "\n",
    "The estimate assumes full price for all prompt tokens, but API providers like Anthropic and OpenAI use **prompt caching** server-side. Since the same system prompt (your CV context) was already sent earlier in this notebook, the provider caches that prefix and charges a reduced rate for subsequent requests.\n",
    "\n",
    "This is good news for the use case: running the same persona against many questions means the context gets cached, and each additional question costs less than the first.\n",
    "\n",
    "**Note:** This is different from LiteLLM's local caching, which can return identical results at zero cost if you re-run the exact same prompt. The provider-side caching still makes API calls but at reduced input token rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run a Survey\n",
    "\n",
    "Run multiple questions together as a survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SURVEY COST ESTIMATE\n",
      "============================================================\n",
      "Questions:                   10\n",
      "Prompt tokens per agent:     18,417\n",
      "Est. completion tokens:      50\n",
      "Est. cost per agent:         $0.0469\n",
      "\n",
      "Scale projections:\n",
      "     10 agents: $0.47\n",
      "    100 agents: $4.69\n",
      "   1000 agents: $46.92\n"
     ]
    }
   ],
   "source": [
    "# Define the survey\n",
    "mini_survey = Survey(\n",
    "    id=\"persona_validation\",\n",
    "    name=\"Persona Validation Survey\",\n",
    "    questions=[\n",
    "        # === CV-INFERABLE QUESTIONS ===\n",
    "        Question(\n",
    "            id=\"career_priority\",\n",
    "            text=\"What matters most to you in a job?\",\n",
    "            question_type=\"single_select\",\n",
    "            options=[\"Impact on society\", \"Financial compensation\", \"Learning opportunities\", \"Work-life balance\", \"Autonomy and creativity\"]\n",
    "        ),\n",
    "        Question(\n",
    "            id=\"tech_stance\",\n",
    "            text=\"How do you feel about AI's impact on employment?\",\n",
    "            question_type=\"single_select\",\n",
    "            options=[\"Net positive - creates more jobs than it destroys\", \"Net negative - mass displacement is coming\", \"Neutral - it will transform jobs but balance out\", \"Too early to tell\"]\n",
    "        ),\n",
    "        Question(\n",
    "            id=\"work_style\",\n",
    "            text=\"How do you prefer to approach complex problems?\",\n",
    "            question_type=\"single_select\",\n",
    "            options=[\"Deep solo research then collaborate\", \"Immediate team brainstorming\", \"Build a prototype first, discuss later\", \"Map out the theory before any implementation\"]\n",
    "        ),\n",
    "        Question(\n",
    "            id=\"industry_interest\",\n",
    "            text=\"Which sector would you most like to work in?\",\n",
    "            question_type=\"single_select\",\n",
    "            options=[\"Climate/sustainability tech\", \"Healthcare/biotech\", \"Finance/fintech\", \"Government/public sector\", \"Pure research/academia\"]\n",
    "        ),\n",
    "        Question(\n",
    "            id=\"skill_development\",\n",
    "            text=\"If you had 3 months to learn anything, what would you prioritise?\",\n",
    "            question_type=\"single_select\",\n",
    "            options=[\"Technical depth (new frameworks, languages)\", \"Domain expertise (economics, biology, etc.)\", \"Leadership and management skills\", \"Creative skills (writing, design)\", \"Starting a business\"]\n",
    "        ),\n",
    "        \n",
    "        # === CULTURAL/POLITICAL QUESTIONS ===\n",
    "        Question(\n",
    "            id=\"ubi_stance\",\n",
    "            text=\"What's your view on Universal Basic Income?\",\n",
    "            question_type=\"single_select\",\n",
    "            options=[\"Strongly support - essential for the future\", \"Cautiously support - worth piloting\", \"Skeptical - prefer targeted interventions\", \"Oppose - undermines work incentives\"]\n",
    "        ),\n",
    "        Question(\n",
    "            id=\"privacy_tradeoff\",\n",
    "            text=\"How do you feel about trading personal data for free services?\",\n",
    "            question_type=\"single_select\",\n",
    "            options=[\"Acceptable - fair exchange\", \"Uncomfortable but unavoidable\", \"Strongly oppose - privacy is sacred\", \"Depends entirely on what data and what service\"]\n",
    "        ),\n",
    "        Question(\n",
    "            id=\"institutions_trust\",\n",
    "            text=\"Which institution do you trust most to act in the public interest?\",\n",
    "            question_type=\"single_select\",\n",
    "            options=[\"National government\", \"Local government\", \"Large corporations\", \"Non-profits and NGOs\", \"Academic institutions\", \"None of the above\"]\n",
    "        ),\n",
    "        Question(\n",
    "            id=\"optimism_future\",\n",
    "            text=\"How do you feel about the next 20 years for humanity?\",\n",
    "            question_type=\"single_select\",\n",
    "            options=[\"Very optimistic - best time to be alive\", \"Cautiously optimistic - progress will continue\", \"Anxious - major challenges ahead\", \"Pessimistic - decline seems likely\"]\n",
    "        ),\n",
    "        Question(\n",
    "            id=\"controversy_take\",\n",
    "            text=\"Which controversial opinion are you most sympathetic to?\",\n",
    "            question_type=\"single_select\",\n",
    "            options=[\"Most meetings should be emails\", \"Remote work is strictly better than office work\", \"Cryptocurrency has no legitimate use case\", \"Social media does more harm than good\", \"Economic growth should not be the primary goal\"]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Estimate cost for the full survey\n",
    "survey_estimate = estimate_survey_cost(persona, mini_survey, num_agents=1)\n",
    "\n",
    "print(\"SURVEY COST ESTIMATE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Questions:                   {len(mini_survey.questions)}\")\n",
    "print(f\"Prompt tokens per agent:     {survey_estimate.prompt_tokens:,}\")\n",
    "print(f\"Est. completion tokens:      {survey_estimate.completion_tokens:,}\")\n",
    "print(f\"Est. cost per agent:         ${survey_estimate.cost_per_agent:.4f}\")\n",
    "print()\n",
    "print(\"Scale projections:\")\n",
    "for n in [10, 100, 1000]:\n",
    "    scaled = estimate_survey_cost(persona, mini_survey, num_agents=n)\n",
    "    print(f\"  {n:>5} agents: ${scaled.total_cost:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survey: Persona Validation Survey\n",
      "\n",
      "============================================================\n",
      "CV-INFERABLE QUESTIONS\n",
      "============================================================\n",
      "\n",
      "Q: What matters most to you in a job?\n",
      "A: Impact on society\n",
      "\n",
      "Q: How do you feel about AI's impact on employment?\n",
      "A: Neutral - it will transform jobs but balance out\n",
      "\n",
      "Q: How do you prefer to approach complex problems?\n",
      "A: Deep solo research then collaborate\n",
      "\n",
      "Q: Which sector would you most like to work in?\n",
      "A: Climate/sustainability tech\n",
      "\n",
      "Q: If you had 3 months to learn anything, what would you prioritise?\n",
      "A: Domain expertise (economics, biology, etc.)\n",
      "\n",
      "============================================================\n",
      "CULTURAL/POLITICAL QUESTIONS (harder to infer)\n",
      "============================================================\n",
      "\n",
      "Q: What's your view on Universal Basic Income?\n",
      "A: Cautiously support - worth piloting\n",
      "\n",
      "Q: How do you feel about trading personal data for free services?\n",
      "A: Depends entirely on what data and what service\n",
      "\n",
      "Q: Which institution do you trust most to act in the public interest?\n",
      "A: Non-profits and NGOs\n",
      "\n",
      "Q: How do you feel about the next 20 years for humanity?\n",
      "A: Cautiously optimistic - progress will continue\n",
      "\n",
      "Q: Which controversial opinion are you most sympathetic to?\n",
      "A: Economic growth should not be the primary goal\n",
      "\n",
      "============================================================\n",
      "ACTUAL COST\n",
      "============================================================\n",
      "Total tokens: 18,486\n",
      "Total cost:   $0.0280\n"
     ]
    }
   ],
   "source": [
    "# Run the survey (this is where the API calls happen)\n",
    "survey_response = await run_survey(persona, mini_survey)\n",
    "\n",
    "print(f\"Survey: {mini_survey.name}\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CV-INFERABLE QUESTIONS\")\n",
    "print('='*60)\n",
    "for r in survey_response.responses[:5]:\n",
    "    q = next(q for q in mini_survey.questions if q.id == r.question_id)\n",
    "    print(f\"\\nQ: {q.text}\")\n",
    "    print(f\"A: {r.response}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CULTURAL/POLITICAL QUESTIONS (harder to infer)\")\n",
    "print('='*60)\n",
    "for r in survey_response.responses[5:]:\n",
    "    q = next(q for q in mini_survey.questions if q.id == r.question_id)\n",
    "    print(f\"\\nQ: {q.text}\")\n",
    "    print(f\"A: {r.response}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ACTUAL COST\")\n",
    "print('='*60)\n",
    "print(f\"Total tokens: {survey_response.total_tokens:,}\")\n",
    "print(f\"Total cost:   ${survey_response.total_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Check Accuracy\n",
    "\n",
    "Compare the persona's answers to your actual answers using the interactive widgets below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b65163b6e4443319c9e67a4852eb142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>CV-Inferable Questions</h3>'), VBox(children=(HTML(value='<b>What matters most …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Build question lookup and persona answers lookup\n",
    "question_lookup = {q.id: q for q in mini_survey.questions}\n",
    "persona_answers = {r.question_id: r.response for r in survey_response.responses}\n",
    "\n",
    "# Create dropdown widgets for each question\n",
    "answer_widgets = {}\n",
    "widget_containers = []\n",
    "\n",
    "cv_questions = [\"career_priority\", \"tech_stance\", \"work_style\", \"industry_interest\", \"skill_development\"]\n",
    "\n",
    "# CV-inferable questions section\n",
    "cv_header = widgets.HTML(\"<h3>CV-Inferable Questions</h3>\")\n",
    "widget_containers.append(cv_header)\n",
    "\n",
    "for qid in cv_questions:\n",
    "    q = question_lookup[qid]\n",
    "    label = widgets.HTML(f\"<b>{q.text}</b>\")\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=[\"-- Select your answer --\"] + q.options,\n",
    "        value=\"-- Select your answer --\",\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    answer_widgets[qid] = dropdown\n",
    "    widget_containers.append(widgets.VBox([label, dropdown], layout=widgets.Layout(margin='0 0 15px 0')))\n",
    "\n",
    "# Cultural/political questions section\n",
    "cultural_questions = [\"ubi_stance\", \"privacy_tradeoff\", \"institutions_trust\", \"optimism_future\", \"controversy_take\"]\n",
    "cultural_header = widgets.HTML(\"<h3>Cultural/Political Questions</h3>\")\n",
    "widget_containers.append(cultural_header)\n",
    "\n",
    "for qid in cultural_questions:\n",
    "    q = question_lookup[qid]\n",
    "    label = widgets.HTML(f\"<b>{q.text}</b>\")\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=[\"-- Select your answer --\"] + q.options,\n",
    "        value=\"-- Select your answer --\",\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    answer_widgets[qid] = dropdown\n",
    "    widget_containers.append(widgets.VBox([label, dropdown], layout=widgets.Layout(margin='0 0 15px 0')))\n",
    "\n",
    "# Submit button and output area\n",
    "submit_button = widgets.Button(\n",
    "    description=\"Check Accuracy\",\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='150px', margin='20px 0')\n",
    ")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_submit(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        \n",
    "        # Collect answers\n",
    "        my_answers = {}\n",
    "        unanswered = []\n",
    "        for qid, w in answer_widgets.items():\n",
    "            if w.value != \"-- Select your answer --\":\n",
    "                my_answers[qid] = w.value\n",
    "            else:\n",
    "                unanswered.append(question_lookup[qid].text)\n",
    "        \n",
    "        if unanswered:\n",
    "            print(\"Please answer all questions before submitting.\")\n",
    "            print(f\"\\nUnanswered: {len(unanswered)} questions\")\n",
    "            return\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        cv_correct = 0\n",
    "        cv_total = 0\n",
    "        cultural_correct = 0\n",
    "        cultural_total = 0\n",
    "        \n",
    "        print(\"COMPARISON: Persona vs Reality\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for qid in cv_questions + cultural_questions:\n",
    "            q = question_lookup[qid]\n",
    "            persona_answer = persona_answers[qid]\n",
    "            actual_answer = my_answers[qid]\n",
    "            match = persona_answer == actual_answer\n",
    "            \n",
    "            if qid in cv_questions:\n",
    "                cv_total += 1\n",
    "                if match:\n",
    "                    cv_correct += 1\n",
    "            else:\n",
    "                cultural_total += 1\n",
    "                if match:\n",
    "                    cultural_correct += 1\n",
    "            \n",
    "            status = \"✓\" if match else \"✗\"\n",
    "            print(f\"\\n{status} {q.text}\")\n",
    "            print(f\"   Persona: {persona_answer}\")\n",
    "            print(f\"   Actual:  {actual_answer}\")\n",
    "        \n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(\"ACCURACY SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"CV-inferable questions:  {cv_correct}/{cv_total} ({cv_correct/cv_total:.0%})\")\n",
    "        print(f\"Cultural/political:      {cultural_correct}/{cultural_total} ({cultural_correct/cultural_total:.0%})\")\n",
    "        total = cv_correct + cultural_correct\n",
    "        all_total = cv_total + cultural_total\n",
    "        print(f\"Overall:                 {total}/{all_total} ({total/all_total:.0%})\")\n",
    "\n",
    "submit_button.on_click(on_submit)\n",
    "\n",
    "# Display everything\n",
    "display(widgets.VBox(widget_containers + [submit_button, output_area]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Fresh Questions\n",
    "\n",
    "Let's test the persona with a new set of questions you haven't seen yet. Answer these yourself first, then run the persona and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 8 fresh questions for validation\n"
     ]
    }
   ],
   "source": [
    "# Define fresh questions\n",
    "fresh_questions = [\n",
    "    Question(\n",
    "        id=\"decision_making\",\n",
    "        text=\"When making important decisions, do you rely more on data or intuition?\",\n",
    "        question_type=\"single_select\",\n",
    "        options=[\"Strongly data-driven\", \"Mostly data with some intuition\", \"Equal balance\", \"Mostly intuition with some data\", \"Strongly intuition-driven\"]\n",
    "    ),\n",
    "    Question(\n",
    "        id=\"team_role\",\n",
    "        text=\"In a team project, which role do you naturally gravitate towards?\",\n",
    "        question_type=\"single_select\",\n",
    "        options=[\"The leader who coordinates\", \"The ideas person who innovates\", \"The executor who gets things done\", \"The analyst who evaluates options\", \"The communicator who keeps everyone aligned\"]\n",
    "    ),\n",
    "    Question(\n",
    "        id=\"learning_style\",\n",
    "        text=\"How do you prefer to learn a new technical skill?\",\n",
    "        question_type=\"single_select\",\n",
    "        options=[\"Read documentation thoroughly first\", \"Jump in and learn by doing\", \"Watch tutorials and videos\", \"Take a structured course\", \"Learn from a mentor or colleague\"]\n",
    "    ),\n",
    "    Question(\n",
    "        id=\"risk_appetite\",\n",
    "        text=\"How would you describe your appetite for career risk?\",\n",
    "        question_type=\"single_select\",\n",
    "        options=[\"Very risk-averse - stability is key\", \"Somewhat cautious - calculated risks only\", \"Moderate - willing to take reasonable chances\", \"Risk-tolerant - growth requires discomfort\", \"Risk-seeking - high risk, high reward\"]\n",
    "    ),\n",
    "    Question(\n",
    "        id=\"work_environment\",\n",
    "        text=\"What type of work environment brings out your best work?\",\n",
    "        question_type=\"single_select\",\n",
    "        options=[\"Quiet, focused solo time\", \"Collaborative open spaces\", \"Flexible mix of both\", \"High-pressure deadlines\", \"Autonomous with minimal oversight\"]\n",
    "    ),\n",
    "    Question(\n",
    "        id=\"tech_adoption\",\n",
    "        text=\"How quickly do you adopt new technologies or tools?\",\n",
    "        question_type=\"single_select\",\n",
    "        options=[\"Early adopter - try everything new\", \"Early majority - adopt once proven useful\", \"Late majority - wait until it's standard\", \"Laggard - only when absolutely necessary\"]\n",
    "    ),\n",
    "    Question(\n",
    "        id=\"conflict_resolution\",\n",
    "        text=\"How do you typically handle disagreements at work?\",\n",
    "        question_type=\"single_select\",\n",
    "        options=[\"Direct confrontation to resolve quickly\", \"Seek compromise and middle ground\", \"Avoid conflict, let things settle naturally\", \"Escalate to a third party if needed\", \"Use data and evidence to settle disputes\"]\n",
    "    ),\n",
    "    Question(\n",
    "        id=\"success_definition\",\n",
    "        text=\"How do you primarily define professional success?\",\n",
    "        question_type=\"single_select\",\n",
    "        options=[\"Impact and contribution to society\", \"Financial achievement and security\", \"Recognition and reputation in your field\", \"Personal growth and learning\", \"Work-life balance and wellbeing\"]\n",
    "    ),\n",
    "]\n",
    "\n",
    "fresh_survey = Survey(\n",
    "    id=\"fresh_validation\",\n",
    "    name=\"Fresh Validation Survey\",\n",
    "    questions=fresh_questions\n",
    ")\n",
    "\n",
    "print(f\"Created {len(fresh_questions)} fresh questions for validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Answers First\n",
    "\n",
    "Answer these questions yourself **before** seeing the persona's responses. This prevents bias from seeing what the AI chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d70b1240514fa7afae228bd097b79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Answer these questions as yourself:</h3>'), VBox(children=(HTML(value='<b>When …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create widgets for your answers (answer BEFORE running the persona)\n",
    "fresh_answer_widgets = {}\n",
    "fresh_widget_containers = []\n",
    "\n",
    "fresh_header = widgets.HTML(\"<h3>Answer these questions as yourself:</h3>\")\n",
    "fresh_widget_containers.append(fresh_header)\n",
    "\n",
    "for q in fresh_questions:\n",
    "    label = widgets.HTML(f\"<b>{q.text}</b>\")\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=[\"-- Select your answer --\"] + q.options,\n",
    "        value=\"-- Select your answer --\",\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    fresh_answer_widgets[q.id] = dropdown\n",
    "    fresh_widget_containers.append(widgets.VBox([label, dropdown], layout=widgets.Layout(margin='0 0 15px 0')))\n",
    "\n",
    "# Lock answers button\n",
    "lock_button = widgets.Button(\n",
    "    description=\"Lock My Answers\",\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='150px', margin='20px 0')\n",
    ")\n",
    "lock_output = widgets.Output()\n",
    "\n",
    "user_fresh_answers = {}\n",
    "\n",
    "def on_lock(b):\n",
    "    with lock_output:\n",
    "        clear_output()\n",
    "        unanswered = []\n",
    "        for qid, w in fresh_answer_widgets.items():\n",
    "            if w.value == \"-- Select your answer --\":\n",
    "                q = next(q for q in fresh_questions if q.id == qid)\n",
    "                unanswered.append(q.text)\n",
    "            else:\n",
    "                user_fresh_answers[qid] = w.value\n",
    "        \n",
    "        if unanswered:\n",
    "            print(f\"Please answer all questions first. ({len(unanswered)} remaining)\")\n",
    "            return\n",
    "        \n",
    "        # Disable all dropdowns\n",
    "        for w in fresh_answer_widgets.values():\n",
    "            w.disabled = True\n",
    "        lock_button.disabled = True\n",
    "        \n",
    "        print(\"✓ Answers locked! Now run the next cell to see the persona's responses.\")\n",
    "\n",
    "lock_button.on_click(on_lock)\n",
    "\n",
    "display(widgets.VBox(fresh_widget_containers + [lock_button, lock_output]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Persona\n",
    "\n",
    "After locking your answers, run this cell to see how the persona responds to the same questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running persona on fresh questions...\n",
      "\n",
      "COMPARISON: Fresh Questions\n",
      "============================================================\n",
      "\n",
      "✗ When making important decisions, do you rely more on data or intuition?\n",
      "   Persona: Mostly data with some intuition\n",
      "   Actual:  Equal balance\n",
      "\n",
      "✓ In a team project, which role do you naturally gravitate towards?\n",
      "   Persona: The ideas person who innovates\n",
      "   Actual:  The ideas person who innovates\n",
      "\n",
      "✗ How do you prefer to learn a new technical skill?\n",
      "   Persona: Jump in and learn by doing\n",
      "   Actual:  Watch tutorials and videos\n",
      "\n",
      "✗ How would you describe your appetite for career risk?\n",
      "   Persona: Moderate - willing to take reasonable chances\n",
      "   Actual:  Somewhat cautious - calculated risks only\n",
      "\n",
      "✓ What type of work environment brings out your best work?\n",
      "   Persona: Flexible mix of both\n",
      "   Actual:  Flexible mix of both\n",
      "\n",
      "✗ How quickly do you adopt new technologies or tools?\n",
      "   Persona: Early adopter - try everything new\n",
      "   Actual:  Late majority - wait until it's standard\n",
      "\n",
      "✗ How do you typically handle disagreements at work?\n",
      "   Persona: Use data and evidence to settle disputes\n",
      "   Actual:  Seek compromise and middle ground\n",
      "\n",
      "✓ How do you primarily define professional success?\n",
      "   Persona: Impact and contribution to society\n",
      "   Actual:  Impact and contribution to society\n",
      "\n",
      "============================================================\n",
      "FRESH QUESTIONS ACCURACY\n",
      "============================================================\n",
      "Correct: 3/8 (38%)\n",
      "\n",
      "Cost: $0.0228\n"
     ]
    }
   ],
   "source": [
    "# Run the fresh survey with the persona\n",
    "if not user_fresh_answers:\n",
    "    print(\"Please lock your answers in the previous cell first!\")\n",
    "else:\n",
    "    print(\"Running persona on fresh questions...\")\n",
    "    fresh_response = await run_survey(persona, fresh_survey)\n",
    "    \n",
    "    # Build comparison\n",
    "    fresh_persona_answers = {r.question_id: r.response for r in fresh_response.responses}\n",
    "    \n",
    "    correct = 0\n",
    "    total = len(fresh_questions)\n",
    "    \n",
    "    print(\"\\nCOMPARISON: Fresh Questions\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for q in fresh_questions:\n",
    "        persona_ans = fresh_persona_answers[q.id]\n",
    "        actual_ans = user_fresh_answers[q.id]\n",
    "        match = persona_ans == actual_ans\n",
    "        if match:\n",
    "            correct += 1\n",
    "        \n",
    "        status = \"✓\" if match else \"✗\"\n",
    "        print(f\"\\n{status} {q.text}\")\n",
    "        print(f\"   Persona: {persona_ans}\")\n",
    "        print(f\"   Actual:  {actual_ans}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"FRESH QUESTIONS ACCURACY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Correct: {correct}/{total} ({correct/total:.0%})\")\n",
    "    print(f\"\\nCost: ${fresh_response.total_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "How well did the personas match reality?\n",
    "\n",
    "**Key observations:**\n",
    "- CV-inferable questions (career, work style, skills) should have higher accuracy since the context contains relevant information\n",
    "- Cultural/political questions test whether the persona can extrapolate from limited context\n",
    "- Fresh questions provide a blind test to avoid confirmation bias\n",
    "- The CV vs free-form comparison tests whether structured data produces better personas than unstructured self-description\n",
    "- Response consistency measures how stable the persona's answers are across different phrasings of the same question\n",
    "\n",
    "See `01_notes.md` for detailed commentary on limitations and improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Free-form Context Comparison\n",
    "\n",
    "How much does structured context (like a CV) matter compared to free-form text? \n",
    "\n",
    "In this step, you'll write a brief description of yourself in your own words. We'll create a new persona from this text and compare its accuracy against the CV-based persona on the same questions.\n",
    "\n",
    "**Hypothesis:** The CV provides structured, factual information that may lead to higher accuracy on career-related questions, while free-form text might capture personality and values better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c89e57033a494ab48463019b671f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n<h3>Describe yourself in your own words</h3>\\n<p style=\"color: #666;\">Write a few…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Free-form text input widget\n",
    "freeform_header = widgets.HTML(\"\"\"\n",
    "<h3>Describe yourself in your own words</h3>\n",
    "<p style=\"color: #666;\">Write a few paragraphs about yourself - your background, interests, values, \n",
    "work style, opinions, etc. Don't look at the survey questions while writing this. \n",
    "Aim for 100-300 words.</p>\n",
    "\"\"\")\n",
    "\n",
    "freeform_textarea = widgets.Textarea(\n",
    "    placeholder='Write about yourself here...\\n\\nFor example:\\n- What do you do for work?\\n- What are your interests and passions?\\n- What values guide your decisions?\\n- How would friends describe you?\\n- What are your views on technology, society, work?',\n",
    "    layout=widgets.Layout(width='600px', height='250px')\n",
    ")\n",
    "\n",
    "word_count_label = widgets.HTML(\"<p><i>Word count: 0</i></p>\")\n",
    "\n",
    "def update_word_count(change):\n",
    "    words = len(change['new'].split())\n",
    "    word_count_label.value = f\"<p><i>Word count: {words}</i></p>\"\n",
    "\n",
    "freeform_textarea.observe(update_word_count, names='value')\n",
    "\n",
    "# Store the context when submitted\n",
    "freeform_context = {\"text\": \"\"}\n",
    "\n",
    "submit_freeform_button = widgets.Button(\n",
    "    description=\"Create Persona\",\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='150px', margin='10px 0')\n",
    ")\n",
    "freeform_output = widgets.Output()\n",
    "\n",
    "def on_submit_freeform(b):\n",
    "    with freeform_output:\n",
    "        clear_output()\n",
    "        text = freeform_textarea.value.strip()\n",
    "        \n",
    "        if len(text.split()) < 20:\n",
    "            print(\"Please write at least 20 words to create a meaningful persona.\")\n",
    "            return\n",
    "        \n",
    "        freeform_context[\"text\"] = text\n",
    "        freeform_textarea.disabled = True\n",
    "        submit_freeform_button.disabled = True\n",
    "        \n",
    "        print(f\"✓ Context saved! ({len(text.split())} words)\")\n",
    "        print(\"\\nRun the next cell to create the persona and compare accuracy.\")\n",
    "\n",
    "submit_freeform_button.on_click(on_submit_freeform)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    freeform_header,\n",
    "    freeform_textarea,\n",
    "    word_count_label,\n",
    "    submit_freeform_button,\n",
    "    freeform_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created persona from free-form text\n",
      "Context length: 20 words\n",
      "(CV persona had: 1149 words)\n",
      "\n",
      "Running survey on free-form persona...\n",
      "✓ Survey complete!\n"
     ]
    }
   ],
   "source": [
    "# Create persona from free-form text and run the survey\n",
    "if not freeform_context[\"text\"]:\n",
    "    print(\"Please submit your free-form description in the previous cell first!\")\n",
    "else:\n",
    "    # Create the free-form persona\n",
    "    freeform_persona = create_persona(\n",
    "        name=\"Free-form Persona\",\n",
    "        context=freeform_context[\"text\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"Created persona from free-form text\")\n",
    "    print(f\"Context length: {len(freeform_persona.context.split())} words\")\n",
    "    print(f\"(CV persona had: {len(persona.context.split())} words)\")\n",
    "    print(\"\\nRunning survey on free-form persona...\")\n",
    "    \n",
    "    # Run the same mini_survey on the free-form persona\n",
    "    freeform_survey_response = await run_survey(freeform_persona, mini_survey)\n",
    "    \n",
    "    print(\"✓ Survey complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Accuracy: CV vs Free-form\n",
    "\n",
    "Now let's compare how well each persona matched your actual answers. You'll need to have completed Step 4 first (where you submitted your actual answers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIDE-BY-SIDE COMPARISON\n",
      "================================================================================\n",
      "Question                            Your Answer          CV       Free-form\n",
      "--------------------------------------------------------------------------------\n",
      "What matters most to you in a job?  Impact on society    ✓        ✓       \n",
      "How do you feel about AI's impact.. Net negative - mas.. ✗        ✗       \n",
      "How do you prefer to approach com.. Immediate team bra.. ✗        ✗       \n",
      "Which sector would you most like .. Healthcare/biotech   ✗        ✗       \n",
      "If you had 3 months to learn anyt.. Domain expertise (.. ✓        ✗       \n",
      "What's your view on Universal Bas.. Strongly support -.. ✗        ✗       \n",
      "How do you feel about trading per.. Acceptable - fair .. ✗        ✗       \n",
      "Which institution do you trust mo.. National government  ✗        ✗       \n",
      "How do you feel about the next 20.. Very optimistic - .. ✗        ✗       \n",
      "Which controversial opinion are y.. Most meetings shou.. ✗        ✓       \n",
      "\n",
      "================================================================================\n",
      "ACCURACY SUMMARY\n",
      "================================================================================\n",
      "Category                  CV Persona           Free-form Persona   \n",
      "--------------------------------------------------------------------------------\n",
      "CV-inferable questions    2/5 (40%)         1/5 (20%)\n",
      "Cultural/political        0/5 (0%)         1/5 (20%)\n",
      "--------------------------------------------------------------------------------\n",
      "OVERALL                   2/10 (20%)         2/10 (20%)\n",
      "\n",
      "================================================================================\n",
      "VERDICT\n",
      "================================================================================\n",
      "Both personas achieved the SAME accuracy!\n",
      "\n",
      "Context comparison:\n",
      "  CV context:        1,149 words\n",
      "  Free-form context: 20 words\n"
     ]
    }
   ],
   "source": [
    "# Compare CV persona vs Free-form persona accuracy\n",
    "# First, collect the user's actual answers from the widgets in Step 4\n",
    "user_actual_answers = {}\n",
    "for qid, w in answer_widgets.items():\n",
    "    if w.value != \"-- Select your answer --\":\n",
    "        user_actual_answers[qid] = w.value\n",
    "\n",
    "if not user_actual_answers:\n",
    "    print(\"Please complete Step 4 first (submit your actual answers using the widgets).\")\n",
    "elif 'freeform_survey_response' not in dir():\n",
    "    print(\"Please run the previous cell first to survey the free-form persona.\")\n",
    "else:\n",
    "    # Build answer lookups\n",
    "    cv_answers = {r.question_id: r.response for r in survey_response.responses}\n",
    "    freeform_answers = {r.question_id: r.response for r in freeform_survey_response.responses}\n",
    "    \n",
    "    # Calculate accuracy for each\n",
    "    cv_cv_correct = 0  # CV persona on CV-inferable questions\n",
    "    cv_cultural_correct = 0\n",
    "    ff_cv_correct = 0  # Free-form persona on CV-inferable questions  \n",
    "    ff_cultural_correct = 0\n",
    "    \n",
    "    print(\"SIDE-BY-SIDE COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Question':<35} {'Your Answer':<20} {'CV':<8} {'Free-form':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    all_questions = cv_questions + cultural_questions\n",
    "    \n",
    "    for qid in all_questions:\n",
    "        if qid not in user_actual_answers:\n",
    "            continue\n",
    "            \n",
    "        q = question_lookup[qid]\n",
    "        actual = user_actual_answers[qid]\n",
    "        cv_ans = cv_answers[qid]\n",
    "        ff_ans = freeform_answers[qid]\n",
    "        \n",
    "        cv_match = cv_ans == actual\n",
    "        ff_match = ff_ans == actual\n",
    "        \n",
    "        # Track scores\n",
    "        if qid in cv_questions:\n",
    "            if cv_match: cv_cv_correct += 1\n",
    "            if ff_match: ff_cv_correct += 1\n",
    "        else:\n",
    "            if cv_match: cv_cultural_correct += 1\n",
    "            if ff_match: ff_cultural_correct += 1\n",
    "        \n",
    "        cv_status = \"✓\" if cv_match else \"✗\"\n",
    "        ff_status = \"✓\" if ff_match else \"✗\"\n",
    "        \n",
    "        # Truncate question text for display\n",
    "        q_short = q.text[:33] + \"..\" if len(q.text) > 35 else q.text\n",
    "        actual_short = actual[:18] + \"..\" if len(actual) > 20 else actual\n",
    "        \n",
    "        print(f\"{q_short:<35} {actual_short:<20} {cv_status:<8} {ff_status:<8}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    cv_total_cv = len([q for q in cv_questions if q in user_actual_answers])\n",
    "    cv_total_cultural = len([q for q in cultural_questions if q in user_actual_answers])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ACCURACY SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Category':<25} {'CV Persona':<20} {'Free-form Persona':<20}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    if cv_total_cv > 0:\n",
    "        cv_pct = cv_cv_correct / cv_total_cv\n",
    "        ff_pct = ff_cv_correct / cv_total_cv\n",
    "        print(f\"{'CV-inferable questions':<25} {cv_cv_correct}/{cv_total_cv} ({cv_pct:.0%}){'':<8} {ff_cv_correct}/{cv_total_cv} ({ff_pct:.0%})\")\n",
    "    \n",
    "    if cv_total_cultural > 0:\n",
    "        cv_pct = cv_cultural_correct / cv_total_cultural\n",
    "        ff_pct = ff_cultural_correct / cv_total_cultural\n",
    "        print(f\"{'Cultural/political':<25} {cv_cultural_correct}/{cv_total_cultural} ({cv_pct:.0%}){'':<8} {ff_cultural_correct}/{cv_total_cultural} ({ff_pct:.0%})\")\n",
    "    \n",
    "    total_cv = cv_cv_correct + cv_cultural_correct\n",
    "    total_ff = ff_cv_correct + ff_cultural_correct\n",
    "    total_q = cv_total_cv + cv_total_cultural\n",
    "    \n",
    "    if total_q > 0:\n",
    "        print(\"-\" * 80)\n",
    "        cv_pct = total_cv / total_q\n",
    "        ff_pct = total_ff / total_q\n",
    "        print(f\"{'OVERALL':<25} {total_cv}/{total_q} ({cv_pct:.0%}){'':<8} {total_ff}/{total_q} ({ff_pct:.0%})\")\n",
    "        \n",
    "        # Verdict\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"VERDICT\")\n",
    "        print(\"=\" * 80)\n",
    "        if total_cv > total_ff:\n",
    "            diff = total_cv - total_ff\n",
    "            print(f\"CV persona was MORE accurate by {diff} question(s)\")\n",
    "        elif total_ff > total_cv:\n",
    "            diff = total_ff - total_cv\n",
    "            print(f\"Free-form persona was MORE accurate by {diff} question(s)\")\n",
    "        else:\n",
    "            print(\"Both personas achieved the SAME accuracy!\")\n",
    "        \n",
    "        print(f\"\\nContext comparison:\")\n",
    "        print(f\"  CV context:        {len(persona.context.split()):,} words\")\n",
    "        print(f\"  Free-form context: {len(freeform_persona.context.split()):,} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Response Consistency Test\n",
    "\n",
    "A reliable persona should give consistent answers when asked the same question in different ways. This step tests **response consistency** - the percentage of times the persona gives the same answer when the same underlying question is rephrased.\n",
    "\n",
    "We'll:\n",
    "1. Define 10 base questions\n",
    "2. Create 10 phrasings of each question (same meaning, different wording)\n",
    "3. Run all 100 questions through the persona\n",
    "4. Calculate consistency per question and overall\n",
    "\n",
    "**Why this matters:** Low consistency suggests the persona's answers are sensitive to question wording rather than reflecting stable preferences. High consistency indicates robust, reliable responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consistency test configuration:\n",
      "  Base questions:     10\n",
      "  Phrasings each:     10\n",
      "  Total API calls:    100\n"
     ]
    }
   ],
   "source": [
    "# Define 10 base questions, each with 10 different phrasings\n",
    "# All phrasings share the same options to enable direct comparison\n",
    "\n",
    "consistency_questions = {\n",
    "    \"work_motivation\": {\n",
    "        \"options\": [\"Money and financial security\", \"Making a positive impact\", \"Personal growth and learning\", \"Recognition and status\", \"Work-life balance\"],\n",
    "        \"phrasings\": [\n",
    "            \"What motivates you most in your career?\",\n",
    "            \"What's the primary driver of your professional life?\",\n",
    "            \"When choosing a job, what matters most to you?\",\n",
    "            \"What gets you out of bed for work each morning?\",\n",
    "            \"What's your main motivation at work?\",\n",
    "            \"What do you value most in your professional life?\",\n",
    "            \"What's the biggest factor in your career satisfaction?\",\n",
    "            \"What drives your career decisions?\",\n",
    "            \"What's most important to you in a job?\",\n",
    "            \"What keeps you engaged in your work?\",\n",
    "        ]\n",
    "    },\n",
    "    \"weekend_preference\": {\n",
    "        \"options\": [\"Socializing with friends\", \"Quiet time alone\", \"Outdoor activities\", \"Creative hobbies\", \"Productive tasks and errands\"],\n",
    "        \"phrasings\": [\n",
    "            \"How do you prefer to spend your weekends?\",\n",
    "            \"What's your ideal weekend activity?\",\n",
    "            \"When Saturday comes, what do you usually do?\",\n",
    "            \"How do you typically unwind on weekends?\",\n",
    "            \"What's your go-to weekend plan?\",\n",
    "            \"How do you like to spend your free time on weekends?\",\n",
    "            \"What does a perfect weekend look like for you?\",\n",
    "            \"When you have time off, how do you spend it?\",\n",
    "            \"What's your preferred way to enjoy the weekend?\",\n",
    "            \"How do you usually occupy your weekends?\",\n",
    "        ]\n",
    "    },\n",
    "    \"conflict_approach\": {\n",
    "        \"options\": [\"Address it directly and immediately\", \"Take time to cool off first\", \"Seek a mediator or third party\", \"Avoid confrontation if possible\", \"Focus on finding compromise\"],\n",
    "        \"phrasings\": [\n",
    "            \"How do you typically handle conflict?\",\n",
    "            \"What's your approach when disagreements arise?\",\n",
    "            \"How do you deal with confrontation?\",\n",
    "            \"When conflict occurs, what do you do?\",\n",
    "            \"What's your conflict resolution style?\",\n",
    "            \"How do you respond to disagreements?\",\n",
    "            \"What's your strategy for handling disputes?\",\n",
    "            \"When you're in conflict, how do you react?\",\n",
    "            \"How do you navigate disagreements with others?\",\n",
    "            \"What's your usual response to conflict situations?\",\n",
    "        ]\n",
    "    },\n",
    "    \"learning_method\": {\n",
    "        \"options\": [\"Reading books and articles\", \"Hands-on experimentation\", \"Video tutorials and courses\", \"Discussion with experts\", \"Structured formal education\"],\n",
    "        \"phrasings\": [\n",
    "            \"How do you prefer to learn new things?\",\n",
    "            \"What's your ideal learning method?\",\n",
    "            \"How do you best absorb new information?\",\n",
    "            \"What's your preferred way to acquire new skills?\",\n",
    "            \"How do you typically approach learning?\",\n",
    "            \"What learning style works best for you?\",\n",
    "            \"How do you like to pick up new knowledge?\",\n",
    "            \"What's your go-to method for learning?\",\n",
    "            \"How do you prefer to be taught new things?\",\n",
    "            \"What's your most effective learning approach?\",\n",
    "        ]\n",
    "    },\n",
    "    \"decision_style\": {\n",
    "        \"options\": [\"Careful analysis of all options\", \"Trust my gut instinct\", \"Seek advice from others\", \"Make quick decisions and adapt\", \"Delay until absolutely necessary\"],\n",
    "        \"phrasings\": [\n",
    "            \"How do you make important decisions?\",\n",
    "            \"What's your decision-making style?\",\n",
    "            \"How do you approach big choices?\",\n",
    "            \"What's your process for making decisions?\",\n",
    "            \"How do you typically decide on important matters?\",\n",
    "            \"What's your approach to decision-making?\",\n",
    "            \"How do you handle major decisions?\",\n",
    "            \"What's your strategy when facing tough choices?\",\n",
    "            \"How do you go about making significant decisions?\",\n",
    "            \"What's your method for reaching important decisions?\",\n",
    "        ]\n",
    "    },\n",
    "    \"stress_response\": {\n",
    "        \"options\": [\"Exercise or physical activity\", \"Talk to friends or family\", \"Spend time alone to recharge\", \"Distract myself with entertainment\", \"Work through it systematically\"],\n",
    "        \"phrasings\": [\n",
    "            \"How do you cope with stress?\",\n",
    "            \"What do you do when you're stressed?\",\n",
    "            \"How do you handle stressful situations?\",\n",
    "            \"What's your go-to stress relief method?\",\n",
    "            \"How do you manage stress in your life?\",\n",
    "            \"What helps you deal with stress?\",\n",
    "            \"How do you typically respond to stress?\",\n",
    "            \"What's your strategy for handling stress?\",\n",
    "            \"How do you unwind when stressed?\",\n",
    "            \"What do you do to relieve stress?\",\n",
    "        ]\n",
    "    },\n",
    "    \"team_preference\": {\n",
    "        \"options\": [\"Lead and direct the team\", \"Collaborate as an equal member\", \"Work independently within the team\", \"Support and help others succeed\", \"Focus on specialized expertise\"],\n",
    "        \"phrasings\": [\n",
    "            \"What role do you prefer in a team?\",\n",
    "            \"How do you like to work in group settings?\",\n",
    "            \"What's your preferred team dynamic?\",\n",
    "            \"How do you typically function in a team?\",\n",
    "            \"What role suits you best in collaborative work?\",\n",
    "            \"How do you prefer to contribute to a team?\",\n",
    "            \"What's your ideal position in a group project?\",\n",
    "            \"How do you like to participate in team efforts?\",\n",
    "            \"What team role do you gravitate towards?\",\n",
    "            \"How do you prefer to engage in teamwork?\",\n",
    "        ]\n",
    "    },\n",
    "    \"change_attitude\": {\n",
    "        \"options\": [\"Embrace it enthusiastically\", \"Accept it cautiously\", \"Resist unless necessary\", \"Adapt quickly and move on\", \"Analyze before responding\"],\n",
    "        \"phrasings\": [\n",
    "            \"How do you respond to change?\",\n",
    "            \"What's your attitude towards change?\",\n",
    "            \"How do you handle unexpected changes?\",\n",
    "            \"What's your reaction when things change?\",\n",
    "            \"How do you deal with change in your life?\",\n",
    "            \"What's your approach to handling change?\",\n",
    "            \"How do you typically respond to new situations?\",\n",
    "            \"What's your attitude when facing change?\",\n",
    "            \"How do you adapt to changes?\",\n",
    "            \"What's your typical response to change?\",\n",
    "        ]\n",
    "    },\n",
    "    \"success_measure\": {\n",
    "        \"options\": [\"Financial achievement\", \"Personal happiness\", \"Impact on others\", \"Professional recognition\", \"Work-life balance\"],\n",
    "        \"phrasings\": [\n",
    "            \"How do you measure success?\",\n",
    "            \"What does success mean to you?\",\n",
    "            \"How do you define personal success?\",\n",
    "            \"What's your measure of a successful life?\",\n",
    "            \"How do you know when you've succeeded?\",\n",
    "            \"What indicates success to you?\",\n",
    "            \"How do you evaluate your own success?\",\n",
    "            \"What's your personal definition of success?\",\n",
    "            \"How do you judge whether you're successful?\",\n",
    "            \"What does being successful look like to you?\",\n",
    "        ]\n",
    "    },\n",
    "    \"communication_style\": {\n",
    "        \"options\": [\"Direct and to the point\", \"Diplomatic and tactful\", \"Detailed and thorough\", \"Casual and friendly\", \"Formal and professional\"],\n",
    "        \"phrasings\": [\n",
    "            \"How would you describe your communication style?\",\n",
    "            \"What's your preferred way of communicating?\",\n",
    "            \"How do you typically express yourself?\",\n",
    "            \"What's your communication approach?\",\n",
    "            \"How do you prefer to convey information?\",\n",
    "            \"What's your style when communicating with others?\",\n",
    "            \"How do you usually communicate?\",\n",
    "            \"What communication style fits you best?\",\n",
    "            \"How would others describe your communication?\",\n",
    "            \"What's your natural way of communicating?\",\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "\n",
    "# Count total questions\n",
    "total_base = len(consistency_questions)\n",
    "total_variations = sum(len(q[\"phrasings\"]) for q in consistency_questions.values())\n",
    "print(f\"Consistency test configuration:\")\n",
    "print(f\"  Base questions:     {total_base}\")\n",
    "print(f\"  Phrasings each:     {len(list(consistency_questions.values())[0]['phrasings'])}\")\n",
    "print(f\"  Total API calls:    {total_variations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 10 surveys with 10 questions each\n",
      "\n",
      "Estimated cost: $0.4672 for all 100 questions\n"
     ]
    }
   ],
   "source": [
    "# Build 10 surveys, each containing one phrasing of each base question\n",
    "# This groups questions by survey rather than by base question to reduce prompt caching effects\n",
    "\n",
    "consistency_surveys = []\n",
    "\n",
    "for survey_idx in range(10):\n",
    "    questions = []\n",
    "    for base_id, q_data in consistency_questions.items():\n",
    "        questions.append(Question(\n",
    "            id=f\"{base_id}_v{survey_idx}\",\n",
    "            text=q_data[\"phrasings\"][survey_idx],\n",
    "            question_type=\"single_select\",\n",
    "            options=q_data[\"options\"]\n",
    "        ))\n",
    "    \n",
    "    consistency_surveys.append(Survey(\n",
    "        id=f\"consistency_survey_{survey_idx}\",\n",
    "        name=f\"Consistency Survey {survey_idx + 1}\",\n",
    "        questions=questions\n",
    "    ))\n",
    "\n",
    "print(f\"Created {len(consistency_surveys)} surveys with {len(consistency_surveys[0].questions)} questions each\")\n",
    "\n",
    "# Estimate cost\n",
    "total_questions = sum(len(s.questions) for s in consistency_surveys)\n",
    "single_estimate = estimate_survey_cost(persona, consistency_surveys[0], num_agents=1)\n",
    "print(f\"\\nEstimated cost: ${single_estimate.cost_per_agent * 10:.4f} for all {total_questions} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running consistency test (100 questions across 10 surveys)...\n",
      "============================================================\n",
      "Survey 1/10... done ($0.0254)\n",
      "Survey 2/10... done ($0.0254)\n",
      "Survey 3/10... done ($0.0254)\n",
      "Survey 4/10... done ($0.0254)\n",
      "Survey 5/10... done ($0.0275)\n",
      "Survey 6/10... done ($0.0254)\n",
      "Survey 7/10... done ($0.0254)\n",
      "Survey 8/10... done ($0.0254)\n",
      "Survey 9/10... done ($0.0254)\n",
      "Survey 10/10... done ($0.0254)\n",
      "============================================================\n",
      "Total cost: $0.2563\n"
     ]
    }
   ],
   "source": [
    "# Run all consistency surveys\n",
    "print(\"Running consistency test (100 questions across 10 surveys)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "consistency_results = {}  # base_question_id -> list of responses\n",
    "total_cost = 0\n",
    "\n",
    "for i, survey in enumerate(consistency_surveys):\n",
    "    print(f\"Survey {i + 1}/10...\", end=\" \", flush=True)\n",
    "    response = await run_survey(persona, survey)\n",
    "    total_cost += response.total_cost\n",
    "    \n",
    "    # Collect responses by base question\n",
    "    for r in response.responses:\n",
    "        # Extract base question id (remove _v0, _v1, etc.)\n",
    "        base_id = r.question_id.rsplit(\"_v\", 1)[0]\n",
    "        if base_id not in consistency_results:\n",
    "            consistency_results[base_id] = []\n",
    "        consistency_results[base_id].append(r.response)\n",
    "    \n",
    "    print(f\"done (${response.total_cost:.4f})\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total cost: ${total_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE CONSISTENCY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Question                                       Consistency Most Common Response     \n",
      "--------------------------------------------------------------------------------\n",
      "What role do you prefer in a team?                     60% Collaborate as an equal..\n",
      "How do you respond to change?                          70% Analyze before responding\n",
      "How do you cope with stress?                           80% Work through it systema..\n",
      "How do you typically handle conflict?                  90% Focus on finding compro..\n",
      "How would you describe your communication s..          90% Detailed and thorough    \n",
      "What motivates you most in your career?               100% Making a positive impact \n",
      "How do you prefer to spend your weekends?             100% Creative hobbies         \n",
      "How do you prefer to learn new things?                100% Hands-on experimentation \n",
      "How do you make important decisions?                  100% Careful analysis of all..\n",
      "How do you measure success?                           100% Impact on others         \n",
      "\n",
      "================================================================================\n",
      "CONSISTENCY SUMMARY\n",
      "================================================================================\n",
      "Overall consistency:     89.0%\n",
      "Most consistent:         100% (work_motivation)\n",
      "Least consistent:        60% (team_preference)\n",
      "\n",
      "================================================================================\n",
      "INTERPRETATION\n",
      "================================================================================\n",
      "Good consistency - responses are reasonably stable with some variation.\n"
     ]
    }
   ],
   "source": [
    "# Calculate consistency metrics\n",
    "from collections import Counter\n",
    "\n",
    "print(\"RESPONSE CONSISTENCY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "question_consistency = {}\n",
    "detailed_results = []\n",
    "\n",
    "for base_id, responses in consistency_results.items():\n",
    "    # Count frequency of each response\n",
    "    counter = Counter(responses)\n",
    "    most_common_response, most_common_count = counter.most_common(1)[0]\n",
    "    \n",
    "    # Consistency = % of responses that match the most common response\n",
    "    consistency = most_common_count / len(responses)\n",
    "    question_consistency[base_id] = consistency\n",
    "    \n",
    "    # Get the first phrasing as the \"base question\" label\n",
    "    base_question = consistency_questions[base_id][\"phrasings\"][0]\n",
    "    \n",
    "    detailed_results.append({\n",
    "        \"base_id\": base_id,\n",
    "        \"question\": base_question,\n",
    "        \"consistency\": consistency,\n",
    "        \"most_common\": most_common_response,\n",
    "        \"distribution\": dict(counter),\n",
    "        \"total\": len(responses)\n",
    "    })\n",
    "\n",
    "# Sort by consistency (lowest first to highlight problem areas)\n",
    "detailed_results.sort(key=lambda x: x[\"consistency\"])\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n{'Question':<45} {'Consistency':>12} {'Most Common Response':<25}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for r in detailed_results:\n",
    "    q_short = r[\"question\"][:43] + \"..\" if len(r[\"question\"]) > 45 else r[\"question\"]\n",
    "    mc_short = r[\"most_common\"][:23] + \"..\" if len(r[\"most_common\"]) > 25 else r[\"most_common\"]\n",
    "    pct = f\"{r['consistency']:.0%}\"\n",
    "    print(f\"{q_short:<45} {pct:>12} {mc_short:<25}\")\n",
    "\n",
    "# Overall consistency\n",
    "overall_consistency = sum(question_consistency.values()) / len(question_consistency)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONSISTENCY SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Overall consistency:     {overall_consistency:.1%}\")\n",
    "print(f\"Most consistent:         {max(question_consistency.values()):.0%} ({max(question_consistency, key=question_consistency.get)})\")\n",
    "print(f\"Least consistent:        {min(question_consistency.values()):.0%} ({min(question_consistency, key=question_consistency.get)})\")\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\" * 80)\n",
    "if overall_consistency >= 0.9:\n",
    "    print(\"Excellent consistency - the persona gives highly stable responses.\")\n",
    "elif overall_consistency >= 0.7:\n",
    "    print(\"Good consistency - responses are reasonably stable with some variation.\")\n",
    "elif overall_consistency >= 0.5:\n",
    "    print(\"Moderate consistency - notable variation in responses to rephrased questions.\")\n",
    "else:\n",
    "    print(\"Low consistency - responses vary significantly based on question wording.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency Insights\n",
    "\n",
    "**What affects consistency?**\n",
    "- Questions with clear, factual answers (based on CV content) tend to be more consistent\n",
    "- Abstract or value-based questions may show more variation\n",
    "- Questions where multiple options could reasonably apply tend to be less consistent\n",
    "\n",
    "**Implications for survey design:**\n",
    "- High consistency questions are more reliable for population-level insights\n",
    "- Low consistency questions may need clearer framing or should be weighted differently\n",
    "- Consider running multiple phrasings and taking the modal response for important questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETAILED RESPONSE DISTRIBUTIONS\n",
      "================================================================================\n",
      "Shows all responses given across the 10 phrasings of each question.\n",
      "\n",
      "Q: What role do you prefer in a team?\n",
      "   Consistency: 60%\n",
      "   Responses:\n",
      "       6x | ██████     | Collaborate as an equal member\n",
      "       3x | ███        | Lead and direct the team\n",
      "       1x | █          | Focus on specialized expertise\n",
      "\n",
      "Q: How do you respond to change?\n",
      "   Consistency: 70%\n",
      "   Responses:\n",
      "       7x | ███████    | Analyze before responding\n",
      "       2x | ██         | Adapt quickly and move on\n",
      "       1x | █          | Embrace it enthusiastically\n",
      "\n",
      "Q: How do you cope with stress?\n",
      "   Consistency: 80%\n",
      "   Responses:\n",
      "       8x | ████████   | Work through it systematically\n",
      "       1x | █          | Exercise or physical activity\n",
      "       1x | █          | Spend time alone to recharge\n",
      "\n",
      "Q: How do you typically handle conflict?\n",
      "   Consistency: 90%\n",
      "   Responses:\n",
      "       9x | █████████  | Focus on finding compromise\n",
      "       1x | █          | Address it directly and immediately\n",
      "\n",
      "Q: How would you describe your communication style?\n",
      "   Consistency: 90%\n",
      "   Responses:\n",
      "       9x | █████████  | Detailed and thorough\n",
      "       1x | █          | Diplomatic and tactful\n",
      "\n",
      "Q: What motivates you most in your career?\n",
      "   Consistency: 100%\n",
      "   Responses:\n",
      "      10x | ██████████ | Making a positive impact\n",
      "\n",
      "Q: How do you prefer to spend your weekends?\n",
      "   Consistency: 100%\n",
      "   Responses:\n",
      "      10x | ██████████ | Creative hobbies\n",
      "\n",
      "Q: How do you prefer to learn new things?\n",
      "   Consistency: 100%\n",
      "   Responses:\n",
      "      10x | ██████████ | Hands-on experimentation\n",
      "\n",
      "Q: How do you make important decisions?\n",
      "   Consistency: 100%\n",
      "   Responses:\n",
      "      10x | ██████████ | Careful analysis of all options\n",
      "\n",
      "Q: How do you measure success?\n",
      "   Consistency: 100%\n",
      "   Responses:\n",
      "      10x | ██████████ | Impact on others\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Detailed breakdown - show response distribution for each question\n",
    "print(\"DETAILED RESPONSE DISTRIBUTIONS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Shows all responses given across the 10 phrasings of each question.\\n\")\n",
    "\n",
    "for r in detailed_results:\n",
    "    print(f\"Q: {r['question']}\")\n",
    "    print(f\"   Consistency: {r['consistency']:.0%}\")\n",
    "    print(f\"   Responses:\")\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_dist = sorted(r['distribution'].items(), key=lambda x: -x[1])\n",
    "    for response, count in sorted_dist:\n",
    "        bar = \"█\" * count\n",
    "        print(f\"      {count:>2}x | {bar:<10} | {response}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
