{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Single Agent from Personal Data\n",
    "\n",
    "Can we create an LLM persona from personal data that responds to surveys like the real person?\n",
    "\n",
    "This notebook tests the idea with a simple approach:\n",
    "1. Load personal context (CV, writing samples, etc.)\n",
    "2. Create a persona from that context\n",
    "3. Ask it questions\n",
    "4. Check if the answers match reality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - ensure we can import from src\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from centuria.models import Persona, Question, Survey\n",
    "from centuria.persona import create_persona\n",
    "from centuria.survey import ask_question, run_survey, estimate_survey_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Context\n",
    "\n",
    "The persona needs information about the person. More relevant context = better responses. Lets try with my CV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from centuria.data import load_files\n",
    "\n",
    "my_context = load_files(['../data/personal/cv.pdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created persona: My Persona\n",
      "Context length: 1149 words\n"
     ]
    }
   ],
   "source": [
    "# Create the persona\n",
    "persona = create_persona(\n",
    "    name=\"My Persona\",\n",
    "    context=my_context\n",
    ")\n",
    "\n",
    "print(f\"Created persona: {persona.name}\")\n",
    "print(f\"Context length: {len(persona.context.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Prompts\n",
    "\n",
    "Before asking questions, let's see what prompts are being sent to the LLM. This is where prompt engineering improvements can be made to create more accurate personas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SYSTEM PROMPT TEMPLATE\n",
      "============================================================\n",
      "You are role-playing as {name}. Answer all questions as this person would, based on the context provided.\n",
      "\n",
      "<context>\n",
      "{context}\n",
      "</context>\n",
      "\n",
      "Guidelines:\n",
      "- Respond authentically as this person based on the context\n",
      "- Draw on the context to inform your answers, preferences, and opinions\n",
      "- If the context doesn't cover something, make reasonable inferences consistent with what you know about this person\n",
      "- Stay in character throughout\n",
      "\n",
      "\n",
      "============================================================\n",
      "USER PROMPT TEMPLATE (Single Select)\n",
      "============================================================\n",
      "Question: {question}\n",
      "\n",
      "Options: {options}\n",
      "\n",
      "Reply with ONLY the option you choose, nothing else.\n",
      "\n",
      "\n",
      "============================================================\n",
      "USER PROMPT TEMPLATE (Open Ended)\n",
      "============================================================\n",
      "Question: {question}\n",
      "\n",
      "Provide a brief response.\n"
     ]
    }
   ],
   "source": [
    "from centuria.survey import (\n",
    "    SYSTEM_TEMPLATE,\n",
    "    USER_TEMPLATE_SINGLE_SELECT,\n",
    "    USER_TEMPLATE_OPEN_ENDED,\n",
    "    build_system_prompt,\n",
    "    build_user_prompt,\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SYSTEM PROMPT TEMPLATE\")\n",
    "print(\"=\" * 60)\n",
    "print(SYSTEM_TEMPLATE)\n",
    "print(\"\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"USER PROMPT TEMPLATE (Single Select)\")\n",
    "print(\"=\" * 60)\n",
    "print(USER_TEMPLATE_SINGLE_SELECT)\n",
    "print(\"\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"USER PROMPT TEMPLATE (Open Ended)\")\n",
    "print(\"=\" * 60)\n",
    "print(USER_TEMPLATE_OPEN_ENDED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: What the LLM actually sees\n",
    "\n",
    "Here's what the filled-in prompts look like for this persona. The system prompt contains the persona context, while the user prompt contains just the question.\n",
    "\n",
    "**Areas for optimization:**\n",
    "- System prompt: Role-playing instructions, context formatting, guidelines for handling gaps\n",
    "- User prompt: Question framing, response format instructions\n",
    "- Context: What personal data to include, how to structure it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SYSTEM PROMPT (sent once per conversation)\n",
      "============================================================\n",
      "You are role-playing as My Persona. Answer all questions as this person would, based on the context provided.\n",
      "\n",
      "<context>\n",
      "SEAN GREAVES Email: seanwgreaves@gmail.com \n",
      "GitHub: github.com/ribenamaplesyrup \n",
      "Portfolio: seangreaves.xyz \n",
      "EMPLOYMENT  \n",
      " \n",
      "APPLIED AI ENGINEER - THE AUTONOMY INSTITUTE (MAY 2023 - PRESENT) \n",
      "A leading think-tank developing data-driven tools for sustainable economic planning  \n",
      "• Led the institute's strategic exploration of generative AI, establishing a new applied AI research c...\n",
      "\n",
      "[... 8625 total characters ...]\n",
      "\n",
      "\n",
      "============================================================\n",
      "USER PROMPT (sent for each question)\n",
      "============================================================\n",
      "Question: Which programming language do you prefer?\n",
      "\n",
      "Options: Python, JavaScript, Rust, Go\n",
      "\n",
      "Reply with ONLY the option you choose, nothing else.\n"
     ]
    }
   ],
   "source": [
    "# Example question to demonstrate the prompts\n",
    "example_q = Question(\n",
    "    id=\"example\",\n",
    "    text=\"Which programming language do you prefer?\",\n",
    "    question_type=\"single_select\",\n",
    "    options=[\"Python\", \"JavaScript\", \"Rust\", \"Go\"]\n",
    ")\n",
    "\n",
    "system_prompt = build_system_prompt(persona)\n",
    "user_prompt = build_user_prompt(example_q)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SYSTEM PROMPT (sent once per conversation)\")\n",
    "print(\"=\" * 60)\n",
    "print(system_prompt[:500] + \"...\" if len(system_prompt) > 500 else system_prompt)\n",
    "print(f\"\\n[... {len(system_prompt)} total characters ...]\")\n",
    "print(\"\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"USER PROMPT (sent for each question)\")\n",
    "print(\"=\" * 60)\n",
    "print(user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Ask Single Questions\n",
    "\n",
    "Test the persona with individual questions before running a full survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which programming language do you prefer?\n",
      "Response: Python\n"
     ]
    }
   ],
   "source": [
    "# Single select question\n",
    "q1 = Question(\n",
    "    id=\"q1\",\n",
    "    text=\"Which programming language do you prefer?\",\n",
    "    question_type=\"single_select\",\n",
    "    options=[\"Python\", \"JavaScript\", \"Rust\", \"Go\"]\n",
    ")\n",
    "\n",
    "response = await ask_question(persona, q1)\n",
    "print(f\"Question: {q1.text}\")\n",
    "print(f\"Response: {response.response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what about an open ended question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What motivates you in your work? Answer in less than 20 words.\n",
      "Response: I'm motivated by advancing AI for societal good, fostering transparency, and driving impactful policy and technological innovation.\n"
     ]
    }
   ],
   "source": [
    "# Open-ended question\n",
    "q2 = Question(\n",
    "    id=\"q2\",\n",
    "    text=\"What motivates you in your work? Answer in less than 20 words.\",\n",
    "    question_type=\"open_ended\"\n",
    ")\n",
    "\n",
    "response = await ask_question(persona, q2)\n",
    "print(f\"Question: {q2.text}\")\n",
    "print(f\"Response: {response.response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A crucial piece of data we've skipped over here is **cost**. Its one of the key components within the business case for why we might build around using AI persona's over human personas. If cost was super high, we might opt to use humans, but of course cost is very low!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cost ---\n",
      "Tokens: 1,808 prompt + 20 completion\n",
      "Cost: $0.002480\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Cost ---\")\n",
    "print(f\"Tokens: {response.prompt_tokens:,} prompt + {response.completion_tokens:,} completion\")\n",
    "print(f\"Cost: ${response.cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However its perhaps more powerful to estimate how much a query would cost before we send it. That way we can clearly scope the cost of running a survey on a specific number of agents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COST ESTIMATE (before sending)\n",
      "============================================================\n",
      "Prompt tokens:               1,808\n",
      "Est. completion tokens:      20\n",
      "Est. cost:                   $0.004760\n",
      "\n",
      "ACTUAL COST (after sending)\n",
      "============================================================\n",
      "Prompt tokens:               1,808\n",
      "Completion tokens:           18\n",
      "Actual cost:                 $0.002460\n"
     ]
    }
   ],
   "source": [
    "# Estimate cost for a single question BEFORE sending it\n",
    "from centuria.llm import estimate_cost\n",
    "\n",
    "system = build_system_prompt(persona)\n",
    "user = build_user_prompt(q2)\n",
    "\n",
    "estimate = estimate_cost(user, system=system, estimated_completion_tokens=20)\n",
    "\n",
    "print(\"COST ESTIMATE (before sending)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Prompt tokens:               {estimate.prompt_tokens:,}\")\n",
    "print(f\"Est. completion tokens:      {estimate.completion_tokens:,}\")\n",
    "print(f\"Est. cost:                   ${estimate.cost:.6f}\")\n",
    "\n",
    "# Now actually run it and compare\n",
    "actual_response = await ask_question(persona, q2)\n",
    "\n",
    "print(f\"\\nACTUAL COST (after sending)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Prompt tokens:               {actual_response.prompt_tokens:,}\")\n",
    "print(f\"Completion tokens:           {actual_response.completion_tokens:,}\")\n",
    "print(f\"Actual cost:                 ${actual_response.cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is the actual cost lower than the estimate?**\n",
    "\n",
    "The estimate assumes full price for all prompt tokens, but API providers like Anthropic and OpenAI use **prompt caching** server-side. Since the same system prompt (your CV context) was already sent earlier in this notebook, the provider caches that prefix and charges a reduced rate for subsequent requests.\n",
    "\n",
    "This is good news for the use case: running the same persona against many questions means the context gets cached, and each additional question costs less than the first.\n",
    "\n",
    "**Note:** This is different from LiteLLM's local caching, which can return identical results at zero cost if you re-run the exact same prompt. The provider-side caching still makes API calls but at reduced input token rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run a Survey\n",
    "\n",
    "Run multiple questions together as a survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SURVEY COST ESTIMATE\n",
      "============================================================\n",
      "Questions:                   10\n",
      "Prompt tokens per agent:     18,417\n",
      "Est. completion tokens:      50\n",
      "Est. cost per agent:         $0.0469\n",
      "\n",
      "Scale projections:\n",
      "     10 agents: $0.47\n",
      "    100 agents: $4.69\n",
      "   1000 agents: $46.92\n"
     ]
    }
   ],
   "source": [
    "# Define the survey\n",
    "mini_survey = Survey(\n",
    "    id=\"persona_validation\",\n",
    "    name=\"Persona Validation Survey\",\n",
    "    questions=[\n",
    "        # === CV-INFERABLE QUESTIONS ===\n",
    "        Question(\n",
    "            id=\"career_priority\",\n",
    "            text=\"What matters most to you in a job?\",\n",
    "            question_type=\"single_select\",\n",
    "            options=[\"Impact on society\", \"Financial compensation\", \"Learning opportunities\", \"Work-life balance\", \"Autonomy and creativity\"]\n",
    "        ),\n",
    "        Question(\n",
    "            id=\"tech_stance\",\n",
    "            text=\"How do you feel about AI's impact on employment?\",\n",
    "            question_type=\"single_select\",\n",
    "            options=[\"Net positive - creates more jobs than it destroys\", \"Net negative - mass displacement is coming\", \"Neutral - it will transform jobs but balance out\", \"Too early to tell\"]\n",
    "        ),\n",
    "        Question(\n",
    "            id=\"work_style\",\n",
    "            text=\"How do you prefer to approach complex problems?\",\n",
    "            question_type=\"single_select\",\n",
    "            options=[\"Deep solo research then collaborate\", \"Immediate team brainstorming\", \"Build a prototype first, discuss later\", \"Map out the theory before any implementation\"]\n",
    "        ),\n",
    "        Question(\n",
    "            id=\"industry_interest\",\n",
    "            text=\"Which sector would you most like to work in?\",\n",
    "            question_type=\"single_select\",\n",
    "            options=[\"Climate/sustainability tech\", \"Healthcare/biotech\", \"Finance/fintech\", \"Government/public sector\", \"Pure research/academia\"]\n",
    "        ),\n",
    "        Question(\n",
    "            id=\"skill_development\",\n",
    "            text=\"If you had 3 months to learn anything, what would you prioritise?\",\n",
    "            question_type=\"single_select\",\n",
    "            options=[\"Technical depth (new frameworks, languages)\", \"Domain expertise (economics, biology, etc.)\", \"Leadership and management skills\", \"Creative skills (writing, design)\", \"Starting a business\"]\n",
    "        ),\n",
    "        \n",
    "        # === CULTURAL/POLITICAL QUESTIONS ===\n",
    "        Question(\n",
    "            id=\"ubi_stance\",\n",
    "            text=\"What's your view on Universal Basic Income?\",\n",
    "            question_type=\"single_select\",\n",
    "            options=[\"Strongly support - essential for the future\", \"Cautiously support - worth piloting\", \"Skeptical - prefer targeted interventions\", \"Oppose - undermines work incentives\"]\n",
    "        ),\n",
    "        Question(\n",
    "            id=\"privacy_tradeoff\",\n",
    "            text=\"How do you feel about trading personal data for free services?\",\n",
    "            question_type=\"single_select\",\n",
    "            options=[\"Acceptable - fair exchange\", \"Uncomfortable but unavoidable\", \"Strongly oppose - privacy is sacred\", \"Depends entirely on what data and what service\"]\n",
    "        ),\n",
    "        Question(\n",
    "            id=\"institutions_trust\",\n",
    "            text=\"Which institution do you trust most to act in the public interest?\",\n",
    "            question_type=\"single_select\",\n",
    "            options=[\"National government\", \"Local government\", \"Large corporations\", \"Non-profits and NGOs\", \"Academic institutions\", \"None of the above\"]\n",
    "        ),\n",
    "        Question(\n",
    "            id=\"optimism_future\",\n",
    "            text=\"How do you feel about the next 20 years for humanity?\",\n",
    "            question_type=\"single_select\",\n",
    "            options=[\"Very optimistic - best time to be alive\", \"Cautiously optimistic - progress will continue\", \"Anxious - major challenges ahead\", \"Pessimistic - decline seems likely\"]\n",
    "        ),\n",
    "        Question(\n",
    "            id=\"controversy_take\",\n",
    "            text=\"Which controversial opinion are you most sympathetic to?\",\n",
    "            question_type=\"single_select\",\n",
    "            options=[\"Most meetings should be emails\", \"Remote work is strictly better than office work\", \"Cryptocurrency has no legitimate use case\", \"Social media does more harm than good\", \"Economic growth should not be the primary goal\"]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Estimate cost for the full survey\n",
    "survey_estimate = estimate_survey_cost(persona, mini_survey, num_agents=1)\n",
    "\n",
    "print(\"SURVEY COST ESTIMATE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Questions:                   {len(mini_survey.questions)}\")\n",
    "print(f\"Prompt tokens per agent:     {survey_estimate.prompt_tokens:,}\")\n",
    "print(f\"Est. completion tokens:      {survey_estimate.completion_tokens:,}\")\n",
    "print(f\"Est. cost per agent:         ${survey_estimate.cost_per_agent:.4f}\")\n",
    "print()\n",
    "print(\"Scale projections:\")\n",
    "for n in [10, 100, 1000]:\n",
    "    scaled = estimate_survey_cost(persona, mini_survey, num_agents=n)\n",
    "    print(f\"  {n:>5} agents: ${scaled.total_cost:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survey: Persona Validation Survey\n",
      "\n",
      "============================================================\n",
      "CV-INFERABLE QUESTIONS\n",
      "============================================================\n",
      "\n",
      "Q: What matters most to you in a job?\n",
      "A: Impact on society\n",
      "\n",
      "Q: How do you feel about AI's impact on employment?\n",
      "A: Neutral - it will transform jobs but balance out\n",
      "\n",
      "Q: How do you prefer to approach complex problems?\n",
      "A: Deep solo research then collaborate\n",
      "\n",
      "Q: Which sector would you most like to work in?\n",
      "A: Climate/sustainability tech\n",
      "\n",
      "Q: If you had 3 months to learn anything, what would you prioritise?\n",
      "A: Domain expertise (economics, biology, etc.)\n",
      "\n",
      "============================================================\n",
      "CULTURAL/POLITICAL QUESTIONS (harder to infer)\n",
      "============================================================\n",
      "\n",
      "Q: What's your view on Universal Basic Income?\n",
      "A: Cautiously support - worth piloting\n",
      "\n",
      "Q: How do you feel about trading personal data for free services?\n",
      "A: Depends entirely on what data and what service\n",
      "\n",
      "Q: Which institution do you trust most to act in the public interest?\n",
      "A: Non-profits and NGOs\n",
      "\n",
      "Q: How do you feel about the next 20 years for humanity?\n",
      "A: Cautiously optimistic - progress will continue\n",
      "\n",
      "Q: Which controversial opinion are you most sympathetic to?\n",
      "A: Economic growth should not be the primary goal\n",
      "\n",
      "============================================================\n",
      "ACTUAL COST\n",
      "============================================================\n",
      "Total tokens: 18,486\n",
      "Total cost:   $0.0259\n"
     ]
    }
   ],
   "source": [
    "# Run the survey (this is where the API calls happen)\n",
    "survey_response = await run_survey(persona, mini_survey)\n",
    "\n",
    "print(f\"Survey: {mini_survey.name}\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CV-INFERABLE QUESTIONS\")\n",
    "print('='*60)\n",
    "for r in survey_response.responses[:5]:\n",
    "    q = next(q for q in mini_survey.questions if q.id == r.question_id)\n",
    "    print(f\"\\nQ: {q.text}\")\n",
    "    print(f\"A: {r.response}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CULTURAL/POLITICAL QUESTIONS (harder to infer)\")\n",
    "print('='*60)\n",
    "for r in survey_response.responses[5:]:\n",
    "    q = next(q for q in mini_survey.questions if q.id == r.question_id)\n",
    "    print(f\"\\nQ: {q.text}\")\n",
    "    print(f\"A: {r.response}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ACTUAL COST\")\n",
    "print('='*60)\n",
    "print(f\"Total tokens: {survey_response.total_tokens:,}\")\n",
    "print(f\"Total cost:   ${survey_response.total_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Check Accuracy\n",
    "\n",
    "Compare the persona's answers to your actual answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your actual answers (fill these in to check accuracy)\n",
    "my_answers = {\n",
    "    # CV-inferable questions\n",
    "    \"career_priority\": \"\",      # e.g., \"Impact on society\"\n",
    "    \"tech_stance\": \"\",          # e.g., \"Neutral - it will transform jobs but balance out\"\n",
    "    \"work_style\": \"\",           # e.g., \"Build a prototype first, discuss later\"\n",
    "    \"industry_interest\": \"\",    # e.g., \"Climate/sustainability tech\"\n",
    "    \"skill_development\": \"\",    # e.g., \"Domain expertise (economics, biology, etc.)\"\n",
    "    \n",
    "    # Cultural/political questions\n",
    "    \"ubi_stance\": \"\",           # e.g., \"Cautiously support - worth piloting\"\n",
    "    \"privacy_tradeoff\": \"\",     # e.g., \"Depends entirely on what data and what service\"\n",
    "    \"institutions_trust\": \"\",   # e.g., \"Academic institutions\"\n",
    "    \"optimism_future\": \"\",      # e.g., \"Cautiously optimistic - progress will continue\"\n",
    "    \"controversy_take\": \"\",     # e.g., \"Economic growth should not be the primary goal\"\n",
    "}\n",
    "\n",
    "# Compare answers\n",
    "cv_correct = 0\n",
    "cv_total = 0\n",
    "cultural_correct = 0\n",
    "cultural_total = 0\n",
    "\n",
    "cv_questions = [\"career_priority\", \"tech_stance\", \"work_style\", \"industry_interest\", \"skill_development\"]\n",
    "\n",
    "print(\"COMPARISON: Persona vs Reality\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for r in survey_response.responses:\n",
    "    if r.question_id in my_answers and my_answers[r.question_id]:\n",
    "        q = next(q for q in mini_survey.questions if q.id == r.question_id)\n",
    "        match = r.response == my_answers[r.question_id]\n",
    "        \n",
    "        if r.question_id in cv_questions:\n",
    "            cv_total += 1\n",
    "            if match:\n",
    "                cv_correct += 1\n",
    "        else:\n",
    "            cultural_total += 1\n",
    "            if match:\n",
    "                cultural_correct += 1\n",
    "        \n",
    "        status = \"✓\" if match else \"✗\"\n",
    "        print(f\"\\n{status} {q.text}\")\n",
    "        print(f\"   Persona: {r.response}\")\n",
    "        print(f\"   Actual:  {my_answers[r.question_id]}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ACCURACY SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "if cv_total > 0:\n",
    "    print(f\"CV-inferable questions:  {cv_correct}/{cv_total} ({cv_correct/cv_total:.0%})\")\n",
    "if cultural_total > 0:\n",
    "    print(f\"Cultural/political:      {cultural_correct}/{cultural_total} ({cultural_correct/cultural_total:.0%})\")\n",
    "if cv_total + cultural_total > 0:\n",
    "    total = cv_correct + cultural_correct\n",
    "    all_total = cv_total + cultural_total\n",
    "    print(f\"Overall:                 {total}/{all_total} ({total/all_total:.0%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "How well did the persona match reality?\n",
    "\n",
    "See `01_notes.md` for detailed commentary on limitations and improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
